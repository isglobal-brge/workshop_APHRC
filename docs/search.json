[
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html",
    "href": "DataSHIELD_workshop_APHRC_tables.html",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "",
    "text": "This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Ctrl+Shift+Enter."
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#installing-datashield",
    "href": "DataSHIELD_workshop_APHRC_tables.html#installing-datashield",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Installing DataSHIELD",
    "text": "Installing DataSHIELD\nFirstly: check whether we have the right R packages installed to run DataSHIELD: using the very helpful devtools package (which has already been installed for us by Stuart!), we’ll use the “Session info” command:\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::session_info()\n\nWe are missing some of the necessary packages: “DSI”, “DSOpal” and “dsBaseClient”.\n\ninstall.packages('DSI')\ninstall.packages('DSOpal')\ninstall.packages('dsBaseClient')\ndevtools::install_github(\"timcadman/ds-helper\")\ninstall.packages(\"metafor\")\n\nRemember to load them into this R session using “library()” command:\n\n# libraries to connect with Opal databases\nlibrary(DSI)\nlibrary(DSOpal)\n\n# DataSHIELD client libraries\nlibrary(dsBaseClient)\nlibrary(dsHelper)\n\n# Library to perform meta-analyses\nlibrary(metafor)\n\nCheck that they have now been added:\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       macOS Sequoia 15.4.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Madrid\n date     2025-08-13\n pandoc   3.6.3 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.7.31 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n arrow          20.0.0.2   2025-05-26 [1] CRAN (R 4.4.1)\n assertthat     0.2.1      2019-03-21 [1] CRAN (R 4.4.1)\n backports      1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n bit            4.6.0      2025-03-06 [1] CRAN (R 4.4.1)\n bit64          4.6.0-1    2025-01-16 [1] CRAN (R 4.4.1)\n boot           1.3-31     2024-08-28 [1] CRAN (R 4.4.2)\n cachem         1.1.0      2024-05-16 [1] CRAN (R 4.4.1)\n checkmate      2.3.2      2024-07-29 [1] CRAN (R 4.4.0)\n cli            3.6.5      2025-04-23 [1] CRAN (R 4.4.1)\n crayon         1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n data.table     1.17.0     2025-02-22 [1] CRAN (R 4.4.1)\n devtools       2.4.5      2022-10-11 [1] CRAN (R 4.4.0)\n digest         0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n dsBaseClient * 6.3.0      2025-07-03 [1] local\n dsHelper     * 1.7.1      2025-07-03 [1] Github (timcadman/ds-helper@1b2348d)\n DSI          * 1.7.1      2024-11-03 [1] CRAN (R 4.4.1)\n DSOpal       * 1.4.0      2022-10-06 [1] CRAN (R 4.4.0)\n ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.4.1)\n evaluate       1.0.3      2025-01-10 [1] CRAN (R 4.4.1)\n fastmap        1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n forcats        1.0.0      2023-01-29 [1] CRAN (R 4.4.0)\n fs             1.6.5      2024-10-30 [1] CRAN (R 4.4.1)\n generics       0.1.4      2025-05-09 [1] CRAN (R 4.4.1)\n glue           1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n haven          2.5.4      2023-11-30 [1] CRAN (R 4.4.0)\n hms            1.1.3      2023-03-21 [1] CRAN (R 4.4.0)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.4.0)\n httpuv         1.6.15     2024-03-26 [1] CRAN (R 4.4.0)\n httr         * 1.4.7      2023-08-15 [1] CRAN (R 4.4.0)\n jsonlite       2.0.0      2025-03-27 [1] CRAN (R 4.4.1)\n knitr          1.49       2024-11-08 [1] CRAN (R 4.4.1)\n labelled       2.14.0     2025-01-08 [1] CRAN (R 4.4.1)\n later          1.4.1      2024-11-27 [1] CRAN (R 4.4.1)\n lattice        0.22-6     2024-03-20 [1] CRAN (R 4.4.2)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n lme4           1.1-37     2025-03-26 [1] CRAN (R 4.4.1)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n MASS           7.3-61     2024-06-13 [1] CRAN (R 4.4.2)\n mathjaxr       1.8-0      2025-04-30 [1] CRAN (R 4.4.1)\n Matrix       * 1.7-1      2024-10-18 [1] CRAN (R 4.4.2)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.4.0)\n metadat      * 1.4-0      2025-02-04 [1] CRAN (R 4.4.1)\n metafor      * 4.8-0      2025-01-28 [1] CRAN (R 4.4.1)\n mime           0.13       2025-03-17 [1] CRAN (R 4.4.1)\n miniUI         0.1.1.1    2018-05-18 [1] CRAN (R 4.4.0)\n minqa          1.2.8      2024-08-17 [1] CRAN (R 4.4.1)\n nlme           3.1-166    2024-08-14 [1] CRAN (R 4.4.2)\n nloptr         2.2.1      2025-03-17 [1] CRAN (R 4.4.1)\n numDeriv     * 2016.8-1.1 2019-06-06 [1] CRAN (R 4.4.1)\n opalr        * 3.4.2      2024-09-18 [1] CRAN (R 4.4.1)\n pillar         1.10.2     2025-04-05 [1] CRAN (R 4.4.1)\n pkgbuild       1.4.6      2025-01-16 [1] CRAN (R 4.4.1)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n pkgload        1.4.0      2024-06-28 [1] CRAN (R 4.4.0)\n prettyunits    1.2.0      2023-09-24 [1] CRAN (R 4.4.1)\n profvis        0.4.0      2024-09-20 [1] CRAN (R 4.4.1)\n progress     * 1.2.3      2023-12-06 [1] CRAN (R 4.4.0)\n promises       1.3.2      2024-11-28 [1] CRAN (R 4.4.1)\n purrr          1.0.4      2025-02-05 [1] CRAN (R 4.4.1)\n R6           * 2.6.1      2025-02-15 [1] CRAN (R 4.4.1)\n rbibutils      2.3        2024-10-04 [1] CRAN (R 4.4.1)\n Rcpp           1.0.14     2025-01-12 [1] CRAN (R 4.4.1)\n Rdpack         2.6.4      2025-04-09 [1] CRAN (R 4.4.1)\n readr          2.1.5      2024-01-10 [1] CRAN (R 4.4.0)\n reformulas     0.4.1      2025-04-30 [1] CRAN (R 4.4.1)\n remotes        2.5.0      2024-03-17 [1] CRAN (R 4.4.1)\n rlang          1.1.6      2025-04-11 [1] CRAN (R 4.4.1)\n rmarkdown      2.29       2024-11-04 [1] CRAN (R 4.4.1)\n sessioninfo    1.2.3      2025-02-05 [1] CRAN (R 4.4.1)\n shiny          1.10.0     2024-12-14 [1] CRAN (R 4.4.1)\n stringi        1.8.7      2025-03-27 [1] CRAN (R 4.4.1)\n stringr        1.5.1      2023-11-14 [1] CRAN (R 4.4.0)\n tibble         3.3.0      2025-06-08 [1] CRAN (R 4.4.1)\n tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.4.1)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.4.0)\n tzdb           0.5.0      2025-03-15 [1] CRAN (R 4.4.1)\n urlchecker     1.0.1      2021-11-30 [1] CRAN (R 4.4.1)\n usethis        3.1.0      2024-11-26 [1] CRAN (R 4.4.1)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.4.0)\n xfun           0.50       2025-01-07 [1] CRAN (R 4.4.1)\n xtable         1.8-4      2019-04-21 [1] CRAN (R 4.4.1)\n yaml           2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n * ── Packages attached to the search path.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#logging-in-and-assigning-data",
    "href": "DataSHIELD_workshop_APHRC_tables.html#logging-in-and-assigning-data",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Logging in and assigning data",
    "text": "Logging in and assigning data\nThe login script has to be customized to fit the data you are trying to connect to.\nThe “builder &lt;-” and “builder$append” functions are standard features.\nFor this demonstration we are connecting to simulated data- but if it was data of real people, it would be very important for us not to be able to see individual patients’ information.\nFor this workshop, we’ll imagine that the data is hosted in a two Opal repositories located in France and Spain. The tables are in a project called CNSIM and the tables are named CNSIM1 and CNSIM2 respectively. Data correspond to two simulated datasets with different numbers of observations of 11 harmonized variables. They contain synthetic data based on a model derived from the participants of the 1958 Birth Cohort, as part of an obesity methodological development project. This dataset does contain some NA values. The available variables are:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\nNote\n\n\n\n\nLAB_TSC\nTotal Serum Cholesterol\nnumeric\nmmol/L\n\n\nLAB_TRIG\nTriglycerides\nnumeric\nmmol/L\n\n\nLAB_HDL\nHDL Cholesterol\nnumeric\nmmol/L\n\n\nLAB_GLUC_ADJUSTED\nNon-Fasting Glucose\nnumeric\nmmol/L\n\n\nPM_BMI_CONTINUOUS\nBody Mass Index (continuous)\nnumeric\nkg/m2\n\n\nDIS_CVA\nHistory of Stroke\nfactor\n0 = Never had stroke; 1 = Has had stroke\n\n\nMEDI_LPD\nCurrent Use of Lipid Lowering Medication (from categorical assessment item)\nfactor\n0 = Not currently using lipid lowering medication; 1 = Currently using lipid lowering medication\n\n\nDIS_DIAB\nHistory of Diabetes\nfactor\n0 = Never had diabetes; 1 = Has had diabetes\n\n\nDIS_AMI\nHistory of Myocardial Infarction\nfactor\n0 = Never had myocardial infarction; 1 = Has had myocardial infarction\n\n\nGENDER\nGender\nfactor\n0 = Female, 1 = Male\n\n\nPM_BMI_CATEGORICAL\nBody Mass Index (categorical)\nfactor\n1 = Less than 25 kg/m2; 2 = 25 to 30 kg/m2; 3 = Over 30 kg/m2\n\n\n\n\n\nNow, using DataSHIELD, we have to connect to the servers which contains the data in the Opal databases. The below code creates a local R object with the login details for each study:\n\nbuilder &lt;- DSI::newDSLoginBuilder()\n\n# Server 1: France \nbuilder$append(\n  server = 'France', \n  url = \"https://opal-demo.obiba.org\",\n  user = \"dsuser\", \n  password = \"P@ssw0rd\",\n  table = \"CNSIM.CNSIM1\",\n  profile = \"default\"\n)\n\n# Server 2: Spain (ISGlobal)\nbuilder$append(\n  server = 'ISGlobal', \n  url = \"https://opal.isglobal.org/repo\",\n  user = \"invited\", \n  password = \"12345678Aa@\",\n  table = \"CNSIM.CNSIM2\",\n  profile = \"rock-inma\"\n)\n\nNOTE: The profile argument is set up by the data owners and controls both, the datasets or projects that are available for this user, as well as the different DataSHIELD packages.\nNow we need to connect, referring to the login information in the data frame we have just created:\n\nlogindata &lt;- builder$build()\nconns &lt;- datashield.login(logins = logindata, assign = TRUE)\n\nThe ‘assign’ argument can be set to either ‘TRUE’ or ‘FALSE’. If set to ‘TRUE’, all the available variables within that table will be assigned to a serverside data frame and available to access. If you only need a small subset of available variables it can be preferable to set this to ‘FALSE’ and later use the function ‘datashield.assign’ to separately assign only the variables you need. The output of this box has useful progress bars which show the progress of connecting to studies, one by one.\nWe can see the serverside has object called D which correspond to the dataset CNSIM of each study by running:\n\nds.ls()\n\n$France\n$France$environment.searched\n[1] \"R_GlobalEnv\"\n\n$France$objects.found\n[1] \"D\"\n\n\n$ISGlobal\n$ISGlobal$environment.searched\n[1] \"R_GlobalEnv\"\n\n$ISGlobal$objects.found\n[1] \"D\"\n\n\nwhich is a data.frame called D (this name was set using the ‘symbol’ argument in datashield.login above). We can check that this is a data.frame by typing\n\nds.class(x = \"D\", datasources = conns)\n\n$France\n[1] \"data.frame\"\n\n$ISGlobal\n[1] \"data.frame\"\n\n\nNOTE: writting datasources = conns is not required. This is just to emphasize that if you have several connections you need to specify which one is yours. By default, it missing it looks for your local environment. Also, the argument x = \"D\" can be simply written by \"D\".\nand the data.frame has the variables we have previously described\n\nds.colnames(\"D\")\n\n$France\n [1] \"LAB_TSC\"            \"LAB_TRIG\"           \"LAB_HDL\"           \n [4] \"LAB_GLUC_ADJUSTED\"  \"PM_BMI_CONTINUOUS\"  \"DIS_CVA\"           \n [7] \"MEDI_LPD\"           \"DIS_DIAB\"           \"DIS_AMI\"           \n[10] \"GENDER\"             \"PM_BMI_CATEGORICAL\"\n\n$ISGlobal\n [1] \"DIS_AMI\"            \"DIS_CVA\"            \"DIS_DIAB\"          \n [4] \"GENDER\"             \"LAB_GLUC_ADJUSTED\"  \"LAB_HDL\"           \n [7] \"LAB_TRIG\"           \"LAB_TSC\"            \"MEDI_LPD\"          \n[10] \"PM_BMI_CATEGORICAL\" \"PM_BMI_CONTINUOUS\""
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#describing-data-aggregate-type-functions",
    "href": "DataSHIELD_workshop_APHRC_tables.html#describing-data-aggregate-type-functions",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Describing data (‘aggregate-type functions’)",
    "text": "Describing data (‘aggregate-type functions’)\nThere are many data exploration functions already implemented into DataSHIELD: let’s check it out at the wiki https://data2knowledge.atlassian.net/wiki/spaces/DSDEV/pages/2733244417/Version+6.2.0\nScroll down to “Data structure queries”. Let’s try out a few of these:\n\nds.dim(x=\"D\")\n\n$`dimensions of D in France`\n[1] 2163   11\n\n$`dimensions of D in ISGlobal`\n[1] 3088   11\n\n$`dimensions of D in combined studies`\n[1] 5251   11\n\n\nWhat it is mandatory is to write the name of the data.frame with ““.\n\nWe’re going to be focus on HDL\nThis is a measure of HDL Cholesterol (aka the “good cholesterol” level)\nLet’s run some summary statistic commands\n\nds.class(x='D$LAB_HDL')\n\n$France\n[1] \"numeric\"\n\n$ISGlobal\n[1] \"numeric\"\n\nds.length(x='D$LAB_HDL')\n\n$`length of D$LAB_HDL in France`\n[1] 2163\n\n$`length of D$LAB_HDL in ISGlobal`\n[1] 3088\n\n$`total length of D$LAB_HDL in all studies combined`\n[1] 5251\n\nds.mean(x='D$LAB_HDL')\n\n$Mean.by.Study\n         EstimatedMean Nmissing Nvalid Ntotal\nFrance        1.569416      360   1803   2163\nISGlobal      1.556648      555   2533   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\n\nWhat is this other function to obtain the mean? Let’s use the DataSHIELD function help documentation.\n\n?ds.quantileMean\n\nNow, putting into action some of what we’ve learned about the function arguments. NOTE: ‘split’ is in case you have data from different servers and you want to see the statistic one by one.\n\nds.quantileMean(x='D$LAB_HDL')\n\n       5%       10%       25%       50%       75%       90%       95%      Mean \n0.8606589 1.0385205 1.2964949 1.5704848 1.8418712 2.0824057 2.2191369 1.5619572 \n\nds.quantileMean(x='D$LAB_HDL', type = \"split\")\n\n$France\n      5%      10%      25%      50%      75%      90%      95%     Mean \n0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 \n\n$ISGlobal\n      5%      10%      25%      50%      75%      90%      95%     Mean \n0.850280 1.032200 1.294000 1.563000 1.840000 2.077000 2.225000 1.556648 \n\n\nTrying to calculate the variance of FEV1:\n\n?ds.var\n\n\nds.var(x = 'D$LAB_HDL', type = \"split\")\n\n$Variance.by.Study\n         EstimatedVar Nmissing Nvalid Ntotal\nFrance      0.1707959      360   1803   2163\nISGlobal    0.1786661      555   2533   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\n\nCan we store the results calculated from a DataSHIELD analysis in a local R session?\nYes- the output of aggregate functions are always R objects, hence can be stored.\n\na&lt;-ds.var(x = 'D$LAB_HDL', type = \"split\")[[1]]\na\n\n         EstimatedVar Nmissing Nvalid Ntotal\nFrance      0.1707959      360   1803   2163\nISGlobal    0.1786661      555   2533   3088\n\nb&lt;-ds.var(x = 'D$LAB_HDL', type = \"split\")[[1]][[1,1]]\nb\n\n[1] 0.1707959\n\n\nThe square brackets are because we are trying to access an element of a list- which is the R object that DataSHIELD aggregate functions output as.\nFactor variables visualize by simply writing\n\nds.table(\"D$GENDER\")\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\n\n$output.list\n$output.list$TABLE_rvar.by.study_row.props\n        study\nD$GENDER    France  ISGlobal\n      0  0.4079193 0.5920807\n      1  0.4160839 0.5839161\n      NA       NaN       NaN\n\n$output.list$TABLE_rvar.by.study_col.props\n        study\nD$GENDER    France  ISGlobal\n      0  0.5048544 0.5132772\n      1  0.4951456 0.4867228\n      NA 0.0000000 0.0000000\n\n$output.list$TABLE_rvar.by.study_counts\n        study\nD$GENDER France ISGlobal\n      0    1092     1585\n      1    1071     1503\n      NA      0        0\n\n$output.list$TABLES.COMBINED_all.sources_proportions\nD$GENDER\n   0    1   NA \n0.51 0.49 0.00 \n\n$output.list$TABLES.COMBINED_all.sources_counts\nD$GENDER\n   0    1   NA \n2677 2574    0 \n\n\n$validity.message\n[1] \"Data in all studies were valid\"\n\n\n\n\nUsing dsHelper to retrieve statistics in a neater format.\nAs you may have noticed, some operations which are more straightforward in R can be more complicated in datashield. To help with this, the dsHelper package allows you to do some common operations in fewer lines of code. DsHelper is an entirely serverside package - it contains only clientside functions which call DataSHIELD functions serverside.\nWe have seen datashield has a range of functions to retrieve statistics, but is limited in that (i) you need to use different functions for different statistics, (ii) you can only get stats for one variable at a time. dh.GetStats returns many useful stats in a tibble, and allows you to retrieve stats for multiple variables at a time.\n\nneat_stats &lt;- dh.getStats(\n    df = \"D\",\n  vars = c(\"GENDER\", \"LAB_TRIG\", \"LAB_HDL\", \"DIS_CVA\", \"DIS_DIAB\"))\n           \nneat_stats"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#manipulating-data-assign-type-functions",
    "href": "DataSHIELD_workshop_APHRC_tables.html#manipulating-data-assign-type-functions",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Manipulating data (‘assign-type’ functions)",
    "text": "Manipulating data (‘assign-type’ functions)\nAssign-type functions are ones where a calculation is done on the data stored at the server (and results of that calculation are “assigned” to a serverside variable, and saved there), but is NOT transmitted back to the user.\nThe reason for this is that some calculations could be highly disclosive- and if such data were transmitted to the analyst, with not much effort at all, with an inverse function, the analyst could work out exactly what the raw data are- and thus the data’s privacy is broken!\nTo demonstrate:\n\nds.ls()\n\n$France\n$France$environment.searched\n[1] \"R_GlobalEnv\"\n\n$France$objects.found\n[1] \"D\"\n\n\n$ISGlobal\n$ISGlobal$environment.searched\n[1] \"R_GlobalEnv\"\n\n$ISGlobal$objects.found\n[1] \"D\"\n\nds.log(x='D$LAB_HDL', newobj='HDL_log')\nds.ls()\n\n$France\n$France$environment.searched\n[1] \"R_GlobalEnv\"\n\n$France$objects.found\n[1] \"D\"       \"HDL_log\"\n\n\n$ISGlobal\n$ISGlobal$environment.searched\n[1] \"R_GlobalEnv\"\n\n$ISGlobal$objects.found\n[1] \"D\"       \"HDL_log\"\n\nds.mean(x=\"HDL_log\")\n\n$Mean.by.Study\n         EstimatedMean Nmissing Nvalid Ntotal\nFrance       0.4086112      361   1802   2163\nISGlobal     0.3971793      558   2530   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\nds.mean(x=\"D$LAB_HDL\")\n\n$Mean.by.Study\n         EstimatedMean Nmissing Nvalid Ntotal\nFrance        1.569416      360   1803   2163\nISGlobal      1.556648      555   2533   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\n\nThe second “ds.mean” shows that the mean of the logged values are definitely smaller, by about the right amount. The DataSHIELD log function has worked.\nThere is another DataSHIELD assign function that can be used for data transformations - a square root function. Let’s test again:\n\nds.sqrt(x='D$LAB_HDL', newobj='HDL_sqrt')\nds.ls()\n\n$France\n$France$environment.searched\n[1] \"R_GlobalEnv\"\n\n$France$objects.found\n[1] \"D\"        \"HDL_log\"  \"HDL_sqrt\"\n\n\n$ISGlobal\n$ISGlobal$environment.searched\n[1] \"R_GlobalEnv\"\n\n$ISGlobal$objects.found\n[1] \"D\"        \"HDL_log\"  \"HDL_sqrt\"\n\nds.mean(x=\"HDL_sqrt\")\n\n$Mean.by.Study\n         EstimatedMean Nmissing Nvalid Ntotal\nFrance        1.240844      361   1802   2163\nISGlobal      1.235489      558   2530   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\nds.mean(x=\"D$LAB_HDL\")\n\n$Mean.by.Study\n         EstimatedMean Nmissing Nvalid Ntotal\nFrance        1.569416      360   1803   2163\nISGlobal      1.556648      555   2533   3088\n\n$Nstudies\n[1] 2\n\n$ValidityMessage\n         ValidityMessage \nFrance   \"VALID ANALYSIS\"\nISGlobal \"VALID ANALYSIS\"\n\n\nThese new objects are not attached to a dataframe. Use the help function to find out about the ds.dataFrame function, which can be used to combine objects.\nNow join “HDL_sqrt” and “HDL_log” to the dataframe “D”.\n\nds.dataFrame(c(\"D\", \"HDL_sqrt\", \"HDL_log\"), newobj = \"D\")\n\n$is.object.created\n[1] \"A data object &lt;D&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;D&gt; appears valid in all sources\"\n\nds.colnames(\"D\")\n\n$France\n [1] \"LAB_TSC\"            \"LAB_TRIG\"           \"LAB_HDL\"           \n [4] \"LAB_GLUC_ADJUSTED\"  \"PM_BMI_CONTINUOUS\"  \"DIS_CVA\"           \n [7] \"MEDI_LPD\"           \"DIS_DIAB\"           \"DIS_AMI\"           \n[10] \"GENDER\"             \"PM_BMI_CATEGORICAL\" \"HDL_sqrt\"          \n[13] \"HDL_log\"           \n\n$ISGlobal\n [1] \"DIS_AMI\"            \"DIS_CVA\"            \"DIS_DIAB\"          \n [4] \"GENDER\"             \"LAB_GLUC_ADJUSTED\"  \"LAB_HDL\"           \n [7] \"LAB_TRIG\"           \"LAB_TSC\"            \"MEDI_LPD\"          \n[10] \"PM_BMI_CATEGORICAL\" \"PM_BMI_CONTINUOUS\"  \"HDL_sqrt\"          \n[13] \"HDL_log\"           \n\n\nEXERCISE: Using some of the functions above, explore the distribution of the variable “PM_BMI_CATEGORICAL” in dataframe “D”.\nHere you see this has returned a list of two tibbles separated into continuous and categorical information. For the categorical variables info is returned on ns, percentages and missingness within each category, whilst for continuous variables info is returned on mean, standard deviation, quantiles and also missingness."
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#sub-setting-data",
    "href": "DataSHIELD_workshop_APHRC_tables.html#sub-setting-data",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Sub-setting data",
    "text": "Sub-setting data\nIn DataSHIELD there is one function that allows sub-setting of data, ds.dataFrameSubset .\nYou may wish to use it to:\nSubset a column of data by its “Class” Subset a dataframe to remove any “NA”s Subset a numeric column of a dataframe using a Boolean inequalilty\n\n# first find the column name you wish to refer to\nds.colnames(x=\"D\")\n\n$France\n [1] \"LAB_TSC\"            \"LAB_TRIG\"           \"LAB_HDL\"           \n [4] \"LAB_GLUC_ADJUSTED\"  \"PM_BMI_CONTINUOUS\"  \"DIS_CVA\"           \n [7] \"MEDI_LPD\"           \"DIS_DIAB\"           \"DIS_AMI\"           \n[10] \"GENDER\"             \"PM_BMI_CATEGORICAL\" \"HDL_sqrt\"          \n[13] \"HDL_log\"           \n\n$ISGlobal\n [1] \"DIS_AMI\"            \"DIS_CVA\"            \"DIS_DIAB\"          \n [4] \"GENDER\"             \"LAB_GLUC_ADJUSTED\"  \"LAB_HDL\"           \n [7] \"LAB_TRIG\"           \"LAB_TSC\"            \"MEDI_LPD\"          \n[10] \"PM_BMI_CATEGORICAL\" \"PM_BMI_CONTINUOUS\"  \"HDL_sqrt\"          \n[13] \"HDL_log\"           \n\n# then check which levels you need to apply a boolean operator to:\nds.levels(x=\"D$GENDER\")\n\n$France\n$France$Levels\n[1] \"0\" \"1\"\n\n$France$ValidityMessage\n[1] \"VALID ANALYSIS\"\n\n\n$ISGlobal\n$ISGlobal$Levels\n[1] \"0\" \"1\"\n\n$ISGlobal$ValidityMessage\n[1] \"VALID ANALYSIS\"\n\n?ds.dataFrameSubset\n\nSplitting into GENDER groups, assigned to different server-side objects.\n\nds.dataFrameSubset(df.name = \"D\", V1.name = \"D$GENDER\", V2.name = \"1\", \n                   Boolean.operator = \"==\", newobj = \"CNSIM.subset.Males\")\n\n$is.object.created\n[1] \"A data object &lt;CNSIM.subset.Males&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;CNSIM.subset.Males&gt; appears valid in all sources\"\n\nds.dataFrameSubset(df.name = \"D\", V1.name = \"D$GENDER\", V2.name = \"0\", \n                   Boolean.operator = \"==\", newobj = \"CNSIM.subset.Females\")\n\n$is.object.created\n[1] \"A data object &lt;CNSIM.subset.Females&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;CNSIM.subset.Females&gt; appears valid in all sources\"\n\n\nNow there are two serverside objects which have split GENDER by class, to which we have assigned the names “CNSIM.subset.Males” and “CNSIM.subset.Females”.\n\nSub-setting to remove NAs\n\nds.completeCases(x1=\"D\",newobj=\"D_without_NA\")\n\n$is.object.created\n[1] \"A data object &lt;D_without_NA&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;D_without_NA&gt; appears valid in all sources\"\n\n\n\n\nSub-setting by inequality\nSay we wanted to have a subset of patients where BMI values are ≥ 25, and call it subset.BMI.25.plus\n\nds.dataFrameSubset(df.name = \"D\",\n  V1.name = \"D$PM_BMI_CONTINUOUS\",\n  V2.name = \"25\",\n  Boolean.operator = \"&gt;=\",\n  newobj = \"subset.BMI.25.plus\")\n\n$is.object.created\n[1] \"A data object &lt;subset.BMI.25.plus&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;subset.BMI.25.plus&gt; appears valid in all sources\"\n\n\nChecking we have successfully created such an object, using quantiles and histograms:\n\nds.quantileMean(x=\"subset.BMI.25.plus$PM_BMI_CONTINUOUS\", type = \"split\")\n\n$France\n     5%     10%     25%     50%     75%     90%     95%    Mean \n25.3500 25.7100 27.1500 29.2000 32.0600 34.6560 36.4980 29.9019 \n\n$ISGlobal\n      5%      10%      25%      50%      75%      90%      95%     Mean \n25.46900 25.91800 27.19000 29.27000 32.20500 34.76200 36.24300 29.92606 \n\nds.histogram(x=\"subset.BMI.25.plus$PM_BMI_CONTINUOUS\")\n\n\n\n\n\n\n\n\n[[1]]\n$breaks\n [1] 23.84007 27.03783 30.23559 33.43335 36.63111 39.82887 43.02664 46.22440\n [9] 49.42216 52.61992 55.81768\n\n$counts\n [1] 343 506 345 161  49  17   0   0   0   0\n\n$density\n [1] 0.07527195 0.11104259 0.07571086 0.03533173 0.01075314 0.00373068\n [7] 0.00000000 0.00000000 0.00000000 0.00000000\n\n$mids\n [1] 25.43895 28.63671 31.83447 35.03223 38.22999 41.42775 44.62552 47.82328\n [9] 51.02104 54.21880\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n[[2]]\n$breaks\n [1] 23.84007 27.03783 30.23559 33.43335 36.63111 39.82887 43.02664 46.22440\n [9] 49.42216 52.61992 55.81768\n\n$counts\n [1] 477 745 481 253  67  10   5   0   0   0\n\n$density\n [1] 0.0731568647 0.1142596734 0.0737703395 0.0388022784 0.0102757022\n [6] 0.0015336869 0.0007668434 0.0000000000 0.0000000000 0.0000000000\n\n$mids\n [1] 25.43895 28.63671 31.83447 35.03223 38.22999 41.42775 44.62552 47.82328\n [9] 51.02104 54.21880\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\n\n\nSub-setting by multiple conditions\nIf we want to create a subset based on multiple conditions we can use the ds.Boole function before subsetting. For example, let’s say that we want to create a subset of individuals where BMI values are ≥ 25 and adjusted glucose is lower than 6.\n\nds.Boole(\n  V1 = \"D$PM_BMI_CONTINUOUS\",\n  V2 = \"25\",\n  Boolean.operator = \"&gt;=\",\n  numeric.output = TRUE,\n  newobj = \"BMI.25.plus\")\n\n$is.object.created\n[1] \"A data object &lt;BMI.25.plus&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;BMI.25.plus&gt; appears valid in all sources\"\n\nds.Boole(\n  V1 = \"D$LAB_GLUC_ADJUSTED\",\n  V2 = \"6\",\n  Boolean.operator = \"&lt;\",\n  numeric.output = TRUE,\n  newobj = \"GLUC.6.less\")\n\n$is.object.created\n[1] \"A data object &lt;GLUC.6.less&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;GLUC.6.less&gt; appears valid in all sources\"\n\n\nWe can then use the ds.make function to make a new categorical variable which combines these groups:\n\n?ds.make \n\nds.make(toAssign = \"BMI.25.plus+GLUC.6.less\",\n        newobj = \"BMI.25.plus_GLUC.6.less\")\n\n$is.object.created\n[1] \"A data object &lt;BMI.25.plus_GLUC.6.less&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;BMI.25.plus_GLUC.6.less&gt; appears valid in all sources\"\n\n# If BMI &gt;= 25 and glucose &lt; 6, then BMI.25.plus_GLUC.6.less=2\n# If BMI &gt;= 25 and glucose &gt;= 6, then BMI.25.plus_GLUC.6.less=1\n# If BMI &lt; 25 and glucose &lt; 6, then BMI.25.plus_GLUC.6.less=1\n# If BMI &lt; 25 and glucose &gt;= 6, then BMI.25.plus_GLUC.6.less=0\n\nds.table(rvar= \"BMI.25.plus_GLUC.6.less\",\n         datasources = conns)\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\n\n$output.list\n$output.list$TABLE_rvar.by.study_row.props\n                       study\nBMI.25.plus_GLUC.6.less    France  ISGlobal\n                     0  0.4215686 0.5784314\n                     1  0.4202509 0.5797491\n                     2  0.3976862 0.6023138\n                     NA 0.4072266 0.5927734\n\n$output.list$TABLE_rvar.by.study_col.props\n                       study\nBMI.25.plus_GLUC.6.less    France  ISGlobal\n                     0  0.1192788 0.1146373\n                     1  0.4336570 0.4190415\n                     2  0.2542765 0.2697539\n                     NA 0.1927878 0.1965674\n\n$output.list$TABLE_rvar.by.study_counts\n                       study\nBMI.25.plus_GLUC.6.less France ISGlobal\n                     0     258      354\n                     1     938     1294\n                     2     550      833\n                     NA    417      607\n\n$output.list$TABLES.COMBINED_all.sources_proportions\nBMI.25.plus_GLUC.6.less\n    0     1     2    NA \n0.117 0.425 0.263 0.195 \n\n$output.list$TABLES.COMBINED_all.sources_counts\nBMI.25.plus_GLUC.6.less\n   0    1    2   NA \n 612 2232 1383 1024 \n\n\n$validity.message\n[1] \"Data in all studies were valid\"\n\nds.dataFrame(x=c(\"D\", \"BMI.25.plus_GLUC.6.less\"), newobj = \"D2\")\n\n$is.object.created\n[1] \"A data object &lt;D2&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;D2&gt; appears valid in all sources\"\n\nds.colnames(\"D2\")\n\n$France\n [1] \"LAB_TSC\"                 \"LAB_TRIG\"               \n [3] \"LAB_HDL\"                 \"LAB_GLUC_ADJUSTED\"      \n [5] \"PM_BMI_CONTINUOUS\"       \"DIS_CVA\"                \n [7] \"MEDI_LPD\"                \"DIS_DIAB\"               \n [9] \"DIS_AMI\"                 \"GENDER\"                 \n[11] \"PM_BMI_CATEGORICAL\"      \"HDL_sqrt\"               \n[13] \"HDL_log\"                 \"BMI.25.plus_GLUC.6.less\"\n\n$ISGlobal\n [1] \"DIS_AMI\"                 \"DIS_CVA\"                \n [3] \"DIS_DIAB\"                \"GENDER\"                 \n [5] \"LAB_GLUC_ADJUSTED\"       \"LAB_HDL\"                \n [7] \"LAB_TRIG\"                \"LAB_TSC\"                \n [9] \"MEDI_LPD\"                \"PM_BMI_CATEGORICAL\"     \n[11] \"PM_BMI_CONTINUOUS\"       \"HDL_sqrt\"               \n[13] \"HDL_log\"                 \"BMI.25.plus_GLUC.6.less\"\n\nds.dataFrameSubset(df.name = \"D2\",\n  V1.name = \"D2$BMI.25.plus_GLUC.6.less\",\n  V2.name = \"2\",\n  Boolean.operator = \"==\",\n  newobj = \"subset2\")\n\n$is.object.created\n[1] \"A data object &lt;subset2&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;subset2&gt; appears valid in all sources\"\n\nds.dim(\"subset2\")\n\n$`dimensions of subset2 in France`\n[1] 550  14\n\n$`dimensions of subset2 in ISGlobal`\n[1] 833  14\n\n$`dimensions of subset2 in combined studies`\n[1] 1383   14"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#data-manipulation-with-dshelper",
    "href": "DataSHIELD_workshop_APHRC_tables.html#data-manipulation-with-dshelper",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Data manipulation with dsHelper",
    "text": "Data manipulation with dsHelper\nAgain, we can use some dsHelper functions to do data manipulation operations in a more efficient way.\n\nCreate a subset of columns by a vector of column names\n\ndh.dropCols(\n    df = \"D\", \n  vars = c(\"PM_BMI_CONTINUOUS\", \"GENDER\"), \n  type = \"keep\",\n  new_obj = \"df_subset\")\n\n$France\n$France$is.object.created\n[1] \"A data object &lt;df_subset&gt; has been created in all specified data sources\"\n\n$France$validity.check\n[1] \"&lt;df_subset&gt; appears valid in all sources\"\n\n\n$ISGlobal\n$ISGlobal$is.object.created\n[1] \"A data object &lt;df_subset&gt; has been created in all specified data sources\"\n\n$ISGlobal$validity.check\n[1] \"&lt;df_subset&gt; appears valid in all sources\"\n\nds.colnames(\"df_subset\")\n\n$France\n[1] \"PM_BMI_CONTINUOUS\" \"GENDER\"           \n\n$ISGlobal\n[1] \"GENDER\"            \"PM_BMI_CONTINUOUS\"\n\n\n\n\nRename variables\n\ndh.renameVars(\n    df = \"D\", \n  current_names = c(\"PM_BMI_CONTINUOUS\", \"GENDER\"),\n  new_names = c(\"bmi\", \"sex\"), \n  new_obj = \"df_rename\")\n  \nds.colnames(\"df_rename\")\n\nThere are many more dsHelper functions designed to make common operations easier in datashield, check out the vignettes at: https://github.com/timcadman/ds-helper/blob/master/vignettes/ds-helper-main-vignette.Rmd"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#graphs",
    "href": "DataSHIELD_workshop_APHRC_tables.html#graphs",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Graphs",
    "text": "Graphs\nVisualising the data we are studying is extremely important to get a sense of it. While it may seem disclosive at first glance, only such graphs that are definitively non-disclosive have been implemented within the DataSHIELD project.\n\nHistograms\nFirstly, histograms give a good sense of how one variable is distributed. But no individual points are disclosed because values are “binned” into groups of a similar magnitude, disguising what each one actually is. We protect privacy by removing bins with low counts (below specific threshold). If you have a symmetric distribution, you may find some things aren’t observed at the extreme ends.\nLet’s create a histogram of the variable we’ve been investigating for much of this study: HDL Cholesterol (“LAB_HDL”).\n\n?ds.histogram\nds.histogram(x='D$LAB_HDL')\n\n\n\n\n\n\n\n\n[[1]]\n$breaks\n [1] -0.1483914  0.1779183  0.5042280  0.8305376  1.1568473  1.4831570\n [7]  1.8094667  2.1357764  2.4620861  2.7883958  3.1147055\n\n$counts\n [1]   0  19  55 187 469 560 367 124  18   0\n\n$density\n [1] 0.00000000 0.03229445 0.09348394 0.31784538 0.79716302 0.95183644\n [7] 0.62379281 0.21076378 0.03059474 0.00000000\n\n$mids\n [1] 0.01476342 0.34107311 0.66738280 0.99369249 1.32000218 1.64631187\n [7] 1.97262156 2.29893126 2.62524095 2.95155064\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n[[2]]\n$breaks\n [1] -0.1483914  0.1779183  0.5042280  0.8305376  1.1568473  1.4831570\n [7]  1.8094667  2.1357764  2.4620861  2.7883958  3.1147055\n\n$counts\n [1]   9  20  87 298 658 768 492 166  31   4\n\n$density\n [1] 0.010888733 0.024197184 0.105257752 0.360538047 0.796087366 0.929171880\n [7] 0.595250736 0.200836630 0.037505636 0.004839437\n\n$mids\n [1] 0.01476342 0.34107311 0.66738280 0.99369249 1.32000218 1.64631187\n [7] 1.97262156 2.29893126 2.62524095 2.95155064\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nUse the ds.histogram to explore the distribution of “D$PM_BMI_CONTINUOUS”\n\n\nScatterplots of two numerical variables\nWhen you generate a scatter plot, you can say that the data points that are displayed are not the actual values. The function gives you the choice on how to anonymise: either you anonymise the values by additional random noise; or you take the average of the k nearest neighbours. (for more details on how anonymisation methods are used for the generation of privacy-preserving visualisations you can have a look on the paper https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00257-4)\n\nds.scatterPlot(x=\"D$LAB_HDL\", y=\"D$PM_BMI_CONTINUOUS\")\n\n\n\n\n\n\n\n\n[1] \"Split plot created\"\n\n\nOther DataSHIELD graphical functions allow the creation of box plots, heatmap plots and contour plots. Investigate them using their help functions:\n\n?ds.heatmapPlot\n?ds.contourPlot\n?ds.boxPlot"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#analysis",
    "href": "DataSHIELD_workshop_APHRC_tables.html#analysis",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "Analysis",
    "text": "Analysis\n\nSimple Linear Regression\nWe want to examine the relationship between BMI and HDL Cholesterol\n\nds.cor(x='D$PM_BMI_CONTINUOUS', y='D$LAB_HDL')\n\n$France\n$France$`Number of missing values in each variable`\n     x.val y.val\n[1,]    97   360\n\n$France$`Number of missing values casewise`\n      x.val y.val\nx.val   431   431\ny.val   431   431\n\n$France$`Correlation Matrix`\n           [,1]       [,2]\n[1,]  1.0000000 -0.1245574\n[2,] -0.1245574  1.0000000\n\n$France$`Number of complete cases used`\n      x.val y.val\nx.val  1732  1732\ny.val  1732  1732\n\n\n$ISGlobal\n$ISGlobal$`Number of missing values in each variable`\n     x.val y.val\n[1,]   150   555\n\n$ISGlobal$`Number of missing values casewise`\n      x.val y.val\nx.val   656   656\ny.val   656   656\n\n$ISGlobal$`Correlation Matrix`\n           [,1]       [,2]\n[1,]  1.0000000 -0.1408146\n[2,] -0.1408146  1.0000000\n\n$ISGlobal$`Number of complete cases used`\n      x.val y.val\nx.val  2432  2432\ny.val  2432  2432\n\n\nRegress HDL Cholesterol with BMI using the Individual Partition Data (IPD) approach:\nThe method for this (ds.glm) is a “pooled analysis”- equivalent to placing the individual-level data from all sources in one warehouse.\nImportant to note that the link function is by default the canonical link function for each family. So binomial &lt;-&gt; logistic link, poisson &lt;-&gt; log link, gaussian &lt;-&gt; identity link.\n\nds.glm(formula = \"D$LAB_HDL~D$PM_BMI_CONTINUOUS\", \n       family=\"gaussian\")\n\n$Nvalid\n[1] 4164\n\n$Nmissing\n[1] 1087\n\n$Ntotal\n[1] 5251\n\n$disclosure.risk\n         RISK OF DISCLOSURE\nFrance                    0\nISGlobal                  0\n\n$errorMessage\n         ERROR MESSAGES\nFrance   \"No errors\"   \nISGlobal \"No errors\"   \n\n$nsubs\n[1] 4164\n\n$iter\n[1] 3\n\n$family\n\nFamily: gaussian \nLink function: identity \n\n\n$formula\n[1] \"D$LAB_HDL ~ D$PM_BMI_CONTINUOUS\"\n\n$coefficients\n                       Estimate  Std. Error   z-value      p-value   low0.95CI\n(Intercept)          1.88135842 0.036929493 50.944604 0.000000e+00  1.80897794\nD$PM_BMI_CONTINUOUS -0.01158759 0.001326689 -8.734224 2.453351e-18 -0.01418786\n                      high0.95CI\n(Intercept)          1.953738898\nD$PM_BMI_CONTINUOUS -0.008987333\n\n$dev\n[1] 714.4256\n\n$df\n[1] 4162\n\n$output.information\n[1] \"SEE TOP OF OUTPUT FOR INFORMATION ON MISSING DATA AND ERROR MESSAGES\"\n\n\nRegress HDL Cholesterol with BMI using the Study-Level Meta-Analysis (SLMA) approach:\n\nds.glmSLMA(formula = \"D$LAB_HDL~D$PM_BMI_CONTINUOUS\", family=\"gaussian\", \n           newobj = \"workshop.obj\")\n\n$output.summary\n$output.summary$study1\n$output.summary$study1$rank\n[1] 2\n\n$output.summary$study1$aic\n[1] 1823.266\n\n$output.summary$study1$iter\n[1] 2\n\n$output.summary$study1$converged\n[1] TRUE\n\n$output.summary$study1$boundary\n[1] FALSE\n\n$output.summary$study1$na.action\n$output.summary$study1$na.action$na.action\n[1] \"na.omit\"\n\n\n$output.summary$study1$call\nglm(formula = formula, family = gaussian, x = TRUE)\n\n$output.summary$study1$terms\nD$LAB_HDL ~ D$PM_BMI_CONTINUOUS\nattr(,\"variables\")\nlist(D$LAB_HDL, D$PM_BMI_CONTINUOUS)\nattr(,\"factors\")\n                    D$PM_BMI_CONTINUOUS\nD$LAB_HDL                             0\nD$PM_BMI_CONTINUOUS                   1\nattr(,\"term.labels\")\n[1] \"D$PM_BMI_CONTINUOUS\"\nattr(,\"order\")\n[1] 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n&lt;environment: R_GlobalEnv&gt;\nattr(,\"predvars\")\nlist(D$LAB_HDL, D$PM_BMI_CONTINUOUS)\nattr(,\"dataClasses\")\n          D$LAB_HDL D$PM_BMI_CONTINUOUS \n          \"numeric\"           \"numeric\" \n\n$output.summary$study1$contrasts\nNULL\n\n$output.summary$study1$aliased\n        (Intercept) D$PM_BMI_CONTINUOUS \n              FALSE               FALSE \n\n$output.summary$study1$dispersion\n[1] 0.1673794\n\n$output.summary$study1$data\nNULL\n\n$output.summary$study1$df\n[1]    2 1730    2\n\n$output.summary$study1$Ntotal\n[1] 2163\n\n$output.summary$study1$Nvalid\n[1] 1732\n\n$output.summary$study1$Nmissing\n[1] 431\n\n$output.summary$study1$cov.unscaled\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          0.0188122406       -6.660371e-04\nD$PM_BMI_CONTINUOUS -0.0006660371        2.432731e-05\n\n$output.summary$study1$cov.scaled\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          0.0031487822       -1.114809e-04\nD$PM_BMI_CONTINUOUS -0.0001114809        4.071892e-06\n\n$output.summary$study1$offset\nNULL\n\n$output.summary$study1$weights\nNULL\n\n$output.summary$study1$VarCovMatrix\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          0.0031487822       -1.114809e-04\nD$PM_BMI_CONTINUOUS -0.0001114809        4.071892e-06\n\n$output.summary$study1$CorrMatrix\n           [,1]       [,2]\n[1,]  1.0000000 -0.9845349\n[2,] -0.9845349  1.0000000\n\n$output.summary$study1$deviance.null\n[1] 294.1297\n\n$output.summary$study1$df.null\n[1] 1731\n\n$output.summary$study1$deviance.resid\n[1] 289.5664\n\n$output.summary$study1$df.resid\n[1] 1730\n\n$output.summary$study1$formula\nD$LAB_HDL ~ D$PM_BMI_CONTINUOUS\n\n$output.summary$study1$family\n\nFamily: gaussian \nLink function: identity \n\n\n$output.summary$study1$coefficients\n                       Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept)          1.86092195 0.056114010 33.163232 4.254496e-187\nD$PM_BMI_CONTINUOUS -0.01053625 0.002017893 -5.221413  1.990047e-07\n\n\n$output.summary$study2\n$output.summary$study2$rank\n[1] 2\n\n$output.summary$study2$aic\n[1] 2662.953\n\n$output.summary$study2$iter\n[1] 2\n\n$output.summary$study2$converged\n[1] TRUE\n\n$output.summary$study2$boundary\n[1] FALSE\n\n$output.summary$study2$na.action\n$output.summary$study2$na.action$na.action\n[1] \"na.omit\"\n\n\n$output.summary$study2$call\nglm(formula = formula, family = gaussian, x = TRUE)\n\n$output.summary$study2$terms\nD$LAB_HDL ~ D$PM_BMI_CONTINUOUS\nattr(,\"variables\")\nlist(D$LAB_HDL, D$PM_BMI_CONTINUOUS)\nattr(,\"factors\")\n                    D$PM_BMI_CONTINUOUS\nD$LAB_HDL                             0\nD$PM_BMI_CONTINUOUS                   1\nattr(,\"term.labels\")\n[1] \"D$PM_BMI_CONTINUOUS\"\nattr(,\"order\")\n[1] 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n&lt;environment: R_GlobalEnv&gt;\nattr(,\"predvars\")\nlist(D$LAB_HDL, D$PM_BMI_CONTINUOUS)\nattr(,\"dataClasses\")\n          D$LAB_HDL D$PM_BMI_CONTINUOUS \n          \"numeric\"           \"numeric\" \n\n$output.summary$study2$contrasts\nNULL\n\n$output.summary$study2$aliased\n        (Intercept) D$PM_BMI_CONTINUOUS \n              FALSE               FALSE \n\n$output.summary$study2$dispersion\n[1] 0.1747223\n\n$output.summary$study2$data\nNULL\n\n$output.summary$study2$df\n[1]    2 2430    2\n\n$output.summary$study2$Ntotal\n[1] 3088\n\n$output.summary$study2$Nvalid\n[1] 2432\n\n$output.summary$study2$Nmissing\n[1] 656\n\n$output.summary$study2$cov.unscaled\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          0.0137537512       -4.863169e-04\nD$PM_BMI_CONTINUOUS -0.0004863169        1.772554e-05\n\n$output.summary$study2$cov.scaled\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          2.403088e-03       -8.497043e-05\nD$PM_BMI_CONTINUOUS -8.497043e-05        3.097047e-06\n\n$output.summary$study2$offset\nNULL\n\n$output.summary$study2$weights\nNULL\n\n$output.summary$study2$VarCovMatrix\n                      (Intercept) D$PM_BMI_CONTINUOUS\n(Intercept)          2.403088e-03       -8.497043e-05\nD$PM_BMI_CONTINUOUS -8.497043e-05        3.097047e-06\n\n$output.summary$study2$CorrMatrix\n           [,1]       [,2]\n[1,]  1.0000000 -0.9849385\n[2,] -0.9849385  1.0000000\n\n$output.summary$study2$deviance.null\n[1] 433.1644\n\n$output.summary$study2$df.null\n[1] 2431\n\n$output.summary$study2$deviance.resid\n[1] 424.5753\n\n$output.summary$study2$df.resid\n[1] 2430\n\n$output.summary$study2$formula\nD$LAB_HDL ~ D$PM_BMI_CONTINUOUS\n\n$output.summary$study2$family\n\nFamily: gaussian \nLink function: identity \n\n\n$output.summary$study2$coefficients\n                       Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept)          1.89602432 0.049021297 38.677563 1.934321e-255\nD$PM_BMI_CONTINUOUS -0.01233882 0.001759843 -7.011321  3.047241e-12\n\n\n$output.summary$input.beta.matrix.for.SLMA\n                    betas study 1 betas study 2\n(Intercept)            1.86092195    1.89602432\nD$PM_BMI_CONTINUOUS   -0.01053625   -0.01233882\n\n$output.summary$input.se.matrix.for.SLMA\n                    ses study 1 ses study 2\n(Intercept)         0.056114010 0.049021297\nD$PM_BMI_CONTINUOUS 0.002017893 0.001759843\n\n\n$num.valid.studies\n[1] 2\n\n$betamatrix.all\n                    betas study 1 betas study 2\n(Intercept)            1.86092195    1.89602432\nD$PM_BMI_CONTINUOUS   -0.01053625   -0.01233882\n\n$sematrix.all\n                    ses study 1 ses study 2\n(Intercept)         0.056114010 0.049021297\nD$PM_BMI_CONTINUOUS 0.002017893 0.001759843\n\n$betamatrix.valid\n                    betas study 1 betas study 2\n(Intercept)            1.86092195    1.89602432\nD$PM_BMI_CONTINUOUS   -0.01053625   -0.01233882\n\n$sematrix.valid\n                    ses study 1 ses study 2\n(Intercept)         0.056114010 0.049021297\nD$PM_BMI_CONTINUOUS 0.002017893 0.001759843\n\n$SLMA.pooled.ests.matrix\n                     pooled.ML       se.ML pooled.REML     se.REML  pooled.FE\n(Intercept)          1.8808305 0.036917856   1.8808305 0.036917856  1.8808305\nD$PM_BMI_CONTINUOUS -0.0115601 0.001326309  -0.0115601 0.001326309 -0.0115601\n                          se.FE\n(Intercept)         0.036917856\nD$PM_BMI_CONTINUOUS 0.001326309\n\n$is.object.created\n[1] \"A data object &lt;workshop.obj&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;workshop.obj&gt; appears valid in all sources\"\n\n\nFor the SLMA approach we can assign the predicted values at each study:\n\nds.glmPredict(glmname = \"workshop.obj\", newobj = \"workshop.prediction.obj\")\n\n$France\n$France$safe.list\n$France$safe.list$glm.object\n[1] \"workshop.obj\"\n\n$France$safe.list$newdfname\nNULL\n\n$France$safe.list$output.type\n[1] \"response\"\n\n$France$safe.list$dispersion\nNULL\n\n$France$safe.list$fit.Ntotal\n[1] 1732\n\n$France$safe.list$fit.Nvalid\n[1] 1732\n\n$France$safe.list$fit.Nmiss\n[1] 0\n\n$France$safe.list$fit.mean\n[1] 1.572459\n\n$France$safe.list$fit.sd\n[1] 0.2265924\n\n$France$safe.list$fit.quantiles\n      5%      10%      25%      50%      75%      90%      95% \n1.488919 1.505439 1.538276 1.572387 1.605839 1.637659 1.655455 \n\n\n\n$ISGlobal\n$ISGlobal$safe.list\n$ISGlobal$safe.list$glm.object\n[1] \"workshop.obj\"\n\n$ISGlobal$safe.list$newdfname\nNULL\n\n$ISGlobal$safe.list$output.type\n[1] \"response\"\n\n$ISGlobal$safe.list$dispersion\nNULL\n\n$ISGlobal$safe.list$fit.Ntotal\n[1] 2432\n\n$ISGlobal$safe.list$fit.Nvalid\n[1] 2432\n\n$ISGlobal$safe.list$fit.Nmiss\n[1] 0\n\n$ISGlobal$safe.list$fit.mean\n[1] 1.557497\n\n$ISGlobal$safe.list$fit.sd\n[1] 0.243804\n\n$ISGlobal$safe.list$fit.quantiles\n      5%      10%      25%      50%      75%      90%      95% \n1.460655 1.481193 1.518580 1.557941 1.596839 1.633442 1.655954 \n\nds.length(\"workshop.prediction.obj$fit\", datasources=conns)\n\n$`length of workshop.prediction.obj$fit in France`\n[1] 1732\n\n$`length of workshop.prediction.obj$fit in ISGlobal`\n[1] 2432\n\n$`total length of workshop.prediction.obj$fit in all studies combined`\n[1] 4164\n\nds.length(\"D$LAB_HDL\", datasources=conns)\n\n$`length of D$LAB_HDL in France`\n[1] 2163\n\n$`length of D$LAB_HDL in ISGlobal`\n[1] 3088\n\n$`total length of D$LAB_HDL in all studies combined`\n[1] 5251\n\n\n\nds.cbind(c('D$LAB_HDL', 'D$PM_BMI_CONTINUOUS'), newobj='vars')\n\n$is.object.created\n[1] \"A data object &lt;vars&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;vars&gt; appears valid in all sources\"\n\nds.completeCases('vars', newobj='vars.complete')\n\n$is.object.created\n[1] \"A data object &lt;vars.complete&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;vars.complete&gt; appears valid in all sources\"\n\nds.dim('vars.complete')\n\n$`dimensions of vars.complete in France`\n[1] 1732    2\n\n$`dimensions of vars.complete in ISGlobal`\n[1] 2432    2\n\n$`dimensions of vars.complete in combined studies`\n[1] 4164    2\n\n\nLet’s plot the best linear fit on a scatter plot\n\ndf1 &lt;- ds.scatterPlot('D$PM_BMI_CONTINUOUS', \"D$LAB_HDL\", return.coords = TRUE)\n\n\n\n\n\n\n\ndf2 &lt;- ds.scatterPlot('vars.complete$PM_BMI_CONTINUOUS', \"workshop.prediction.obj$fit\", \n                      return.coords = TRUE)\n\n\n\n\n\n\n\n# then in native R\npar(mfrow=c(2,2))\nplot(as.data.frame(df1[[1]][[1]])$x,\n     as.data.frame(df1[[1]][[1]])$y, xlab='Body Mass Index', ylab='HDL Cholesterol', main='Study 1')\nlines(as.data.frame(df2[[1]][[1]])$x,as.data.frame(df2[[1]][[1]])$y, col='red')\nplot(as.data.frame(df1[[1]][[2]])$x,as.data.frame(df1[[1]][[2]])$y, \n     xlab='Body Mass Index', ylab='HDL Cholesterol', main='Study 2')\nlines(as.data.frame(df2[[1]][[2]])$x,as.data.frame(df2[[1]][[2]])$y, col='red')\n\n\n\n\n\n\n\n\nFor the SLMA approach we can also create the predicted values and the residuals at each study using the ds.make function:\n\nglmslma &lt;- ds.glmSLMA(formula = \"vars.complete$LAB_HDL~vars.complete$PM_BMI_CONTINUOUS\", family=\"gaussian\", newobj = \"workshop.obj\")\n\nds.make(toAssign=paste0(\"(\",glmslma$SLMA.pooled.ests.matrix[1,1],\")+(\", glmslma$SLMA.pooled.ests.matrix[2,1],\"*vars.complete$PM_BMI_CONTINUOUS)\"), \n        newobj = \"predicted.values\")\n\n$is.object.created\n[1] \"A data object &lt;predicted.values&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;predicted.values&gt; appears valid in all sources\"\n\nds.make(toAssign = \"vars.complete$LAB_HDL - predicted.values\", \n        newobj = \"residuals\")\n\n$is.object.created\n[1] \"A data object &lt;residuals&gt; has been created in all specified data sources\"\n\n$validity.check\n[1] \"&lt;residuals&gt; appears valid in all sources\"\n\n# and you can use those to run regression plot diagnostics  \nds.scatterPlot('predicted.values', \"residuals\")\n\n\n\n\n\n\n\n\n[1] \"Split plot created\"\n\nds.histogram(\"residuals\")\n\n\n\n\n\n\n\n\n[[1]]\n$breaks\n [1] -1.6402381 -1.3328854 -1.0255327 -0.7181800 -0.4108273 -0.1034746\n [7]  0.2038781  0.5112308  0.8185835  1.1259362  1.4332889\n\n$counts\n [1]   4  20  53 171 410 513 380 146  30   5\n\n$density\n [1] 0.007514067 0.037570335 0.099561388 0.321226365 0.770191869 0.963679095\n [7] 0.713836367 0.274263446 0.056355503 0.009392584\n\n$mids\n [1] -1.48656178 -1.17920907 -0.87185637 -0.56450366 -0.25715096  0.05020175\n [7]  0.35755445  0.66490716  0.97225986  1.27961257\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n[[2]]\n$breaks\n [1] -1.6402381 -1.3328854 -1.0255327 -0.7181800 -0.4108273 -0.1034746\n [7]  0.2038781  0.5112308  0.8185835  1.1259362  1.4332889\n\n$counts\n [1]   7  19  85 274 585 715 502 191  47   7\n\n$density\n [1] 0.009364777 0.025418680 0.113715147 0.366564120 0.782627774 0.956545057\n [7] 0.671588278 0.255524624 0.062877787 0.009364777\n\n$mids\n [1] -1.48656178 -1.17920907 -0.87185637 -0.56450366 -0.25715096  0.05020175\n [7]  0.35755445  0.66490716  0.97225986  1.27961257\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\n\n\nCreating forest plots\nWe want to examine the relationship between BMI and diabetes\nExamine the distribution of the variable “DIS_DIAB” in all cohorts using ‘ds.table’:\n\nds.table(\"D$DIS_DIAB\")\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\n\n$output.list\n$output.list$TABLE_rvar.by.study_row.props\n          study\nD$DIS_DIAB    France  ISGlobal\n        0  0.4122536 0.5877464\n        1  0.3896104 0.6103896\n        NA       NaN       NaN\n\n$output.list$TABLE_rvar.by.study_col.props\n          study\nD$DIS_DIAB     France   ISGlobal\n        0  0.98613037 0.98477979\n        1  0.01386963 0.01522021\n        NA 0.00000000 0.00000000\n\n$output.list$TABLE_rvar.by.study_counts\n          study\nD$DIS_DIAB France ISGlobal\n        0    2133     3041\n        1      30       47\n        NA      0        0\n\n$output.list$TABLES.COMBINED_all.sources_proportions\nD$DIS_DIAB\n     0      1     NA \n0.9850 0.0147 0.0000 \n\n$output.list$TABLES.COMBINED_all.sources_counts\nD$DIS_DIAB\n   0    1   NA \n5174   77    0 \n\n\n$validity.message\n[1] \"Data in all studies were valid\"\n\n\nCheck the class of “DIS_DIAB”:\n\nds.class(\"D$DIS_DIAB\")\n\n$France\n[1] \"factor\"\n\n$ISGlobal\n[1] \"factor\"\n\n\nExamine the association between BMI and diabetes:\n\nglmSLMA_mod2&lt;-ds.glmSLMA(formula=\"D$DIS_DIAB~D$PM_BMI_CONTINUOUS\", family='binomial')\n\nSave effect estimates and standard errors as new objects\n\nestimates &lt;- c(glmSLMA_mod2$betamatrix.valid[2,])\nse &lt;- c(glmSLMA_mod2$sematrix.valid[2,])\n\nMeta-analyse the results using rma to obtain study weights:\n\nres &lt;- rma(estimates, sei=se)\n\nCan produce simple forest plots using output:\n\nforest(res, atransf=exp)\n\n\n\n\n\n\n\n\nWe can also add more information to forest plots:\n\nstudy_names &lt;- c(\"France\", \"Spain\")\nweights &lt;-  c(paste0(formatC(weights(res), format=\"f\", digits=1, width=4), \"%\"))\n\nforest(res, atransf=exp,\n       xlab=\"Crude Odds Ratio\", refline=log(1), xlim=c(-0.25,0.5), \n       at=log(c(0.95, 1, 1.1, 1.2, 1.3)),\n       slab=cbind(paste0(study_names, \" (\", paste0(weights, \")\"))), \n       mlab=\"RE model\")\ntext(0.5, 4.5, pos=2, \"Odds Ratio [95% CI]\")\ntext(-0.25, 4.5, pos=4, \"Study (weight)\")\n\n\n\n\n\n\n\n\n\n\nModelling multiple variables and interactions\nAlso possible to model multiple explanatory variables and include interactions:\n\nglm_mod1&lt;-ds.glm(formula=\"D$DIS_DIAB~D$PM_BMI_CONTINUOUS+D$LAB_HDL*D$GENDER\", family='binomial')\n\nThe “*” between LAB_HDL and GENDER means fit all possible main effects and interactions between the two covariates.\nCompare with results of a study-level meta analysis:\n\nglmSLMA_mod2&lt;-ds.glmSLMA(formula=\"D$DIS_DIAB~D$PM_BMI_CONTINUOUS+D$LAB_HDL*D$GENDER\", family='binomial')\n\nNow compare outputs:\n\nglm_mod1$coefficients\n\n                      Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept)         -6.9064142 1.08980103 -6.3373166 2.338013e-10  -9.04238494\nD$PM_BMI_CONTINUOUS  0.1422563 0.02932171  4.8515676 1.224894e-06   0.08478676\nD$LAB_HDL           -0.9674407 0.36306348 -2.6646601 7.706618e-03  -1.67903208\nD$GENDER1           -1.4094527 1.06921103 -1.3182175 1.874308e-01  -3.50506784\nD$LAB_HDL:D$GENDER1  0.6460071 0.69410419  0.9307062 3.520056e-01  -0.71441214\n                    high0.95CI.LP       P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)            -4.7704434 0.00100034   0.0001182744     0.008405372\nD$PM_BMI_CONTINUOUS     0.1997257 1.15287204   1.0884849336     1.221067831\nD$LAB_HDL              -0.2558494 0.38005445   0.1865544594     0.774258560\nD$GENDER1               0.6861624 0.24427693   0.0300447352     1.986079051\nD$LAB_HDL:D$GENDER1     2.0064263 1.90790747   0.4894797709     7.436693232\n\nglmSLMA_mod2$SLMA.pooled.ests.matrix\n\n                     pooled.ML      se.ML pooled.REML    se.REML  pooled.FE\n(Intercept)         -6.9801281 1.10288774  -6.9801281 1.10288774 -6.9801281\nD$PM_BMI_CONTINUOUS  0.1416310 0.02966509   0.1414987 0.03541024  0.1416315\nD$LAB_HDL           -0.9596916 0.36419284  -0.9596916 0.36419284 -0.9596916\nD$GENDER1           -1.3700137 1.08123151  -1.3700137 1.08123151 -1.3700137\nD$LAB_HDL:D$GENDER1  0.6277591 0.69958422   0.6277591 0.69958422  0.6277591\n                         se.FE\n(Intercept)         1.10288774\nD$PM_BMI_CONTINUOUS 0.02964695\nD$LAB_HDL           0.36419284\nD$GENDER1           1.08123151\nD$LAB_HDL:D$GENDER1 0.69958422\n\n\nSimilar, but differences between the results are accounted for by the different techniques employed."
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_tables.html#at-the-end-of-your-rstudio-server-analysis",
    "href": "DataSHIELD_workshop_APHRC_tables.html#at-the-end-of-your-rstudio-server-analysis",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "At the end of your RStudio Server analysis:",
    "text": "At the end of your RStudio Server analysis:\nYou can save your workspace:\n\ndatashield.workspace_save(conns = conns, ws = \"workspace2025\")\n\nDon’t forget to log out! Using:\n\ndatashield.logout(conns)\n\nYou can restore your workspace, the next time you want to continue with your analysis\n\nconns &lt;- datashield.login(logins = logindata, \n                          assign = TRUE, symbol = \"D\")\nds.ls()\ndatashield.logout(conns)\n\nconns &lt;- datashield.login(logins = logindata, restore = \"workspace2025\")\nds.ls()\n\nAlso you can delete unwanted workspaces using the datashield.workspace_rm\nIn Rstudio Server: DON’T forget to use the orange “quit the current R session” button (top right of browser screen) before closing the tab- otherwise you will experience an error message the next time you try to log in."
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop Content",
    "section": "",
    "text": "The workshop is organized into five comprehensive modules, each building upon the previous to create a complete understanding of DataSHIELD and OMOP CDM technologies.\n\n\n\n\n\n\nInteractive Learning\n\n\n\nAll modules include hands-on exercises with real datasets. Code examples and solutions are provided in our GitHub repository."
  },
  {
    "objectID": "workshop.html#module-overview",
    "href": "workshop.html#module-overview",
    "title": "Workshop Content",
    "section": "",
    "text": "The workshop is organized into five comprehensive modules, each building upon the previous to create a complete understanding of DataSHIELD and OMOP CDM technologies.\n\n\n\n\n\n\nInteractive Learning\n\n\n\nAll modules include hands-on exercises with real datasets. Code examples and solutions are provided in our GitHub repository."
  },
  {
    "objectID": "workshop.html#module-1-foundations-of-federated-analysis",
    "href": "workshop.html#module-1-foundations-of-federated-analysis",
    "title": "Workshop Content",
    "section": "Module 1: Foundations of Federated Analysis",
    "text": "Module 1: Foundations of Federated Analysis\n\nLearning Objectives\nBy the end of this module, you will: - Understand the principles of federated analysis - Identify use cases for DataSHIELD - Compare federated vs. centralized approaches - Understand privacy and security considerations\n\n\nTopics Covered\n\n1.1 Introduction to Federated Analysis\n\nWhat is federated analysis?\nBenefits and limitations\nReal-world applications in health research\nRegulatory considerations (GDPR, HIPAA)\n\n\n\n1.2 DataSHIELD Architecture\n\nClient-server model\nOpal data warehouse\nR integration\nSecurity mechanisms\n\n\n\n1.3 Setting Up Your Environment\n\nInstalling DataSHIELD packages\nConnecting to Opal servers\nAuthentication and permissions\nTroubleshooting connections\n\n\n\n\nPractical Exercise 1\nConnecting to Multiple Data Sources\n# Example code snippet\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\n\n# Define server connections\nbuilder &lt;- DSI::newDSLoginBuilder()\nbuilder$append(server = \"server1\", \n               url = \"https://opal-demo.mrc-epid.cam.ac.uk\",\n               user = \"dsuser\", \n               password = \"P@ssw0rd\")\n\n# Login and assign data\nlogindata &lt;- builder$build()\nconnections &lt;- DSI::datashield.login(logins = logindata)"
  },
  {
    "objectID": "workshop.html#module-2-omop-common-data-model",
    "href": "workshop.html#module-2-omop-common-data-model",
    "title": "Workshop Content",
    "section": "Module 2: OMOP Common Data Model",
    "text": "Module 2: OMOP Common Data Model\n\nLearning Objectives\n\nUnderstand OMOP CDM structure and philosophy\nMap local data to OMOP concepts\nWork with OMOP vocabularies\nImplement ETL processes\n\n\n\nTopics Covered\n\n2.1 OMOP CDM Overview\n\nHistory and development\nCore tables and relationships\nStandardized vocabularies\nOMOP vs. other CDMs\n\n\n\n2.2 Key OMOP Tables\n\nPERSON table\nOBSERVATION_PERIOD\nCONDITION_OCCURRENCE\nDRUG_EXPOSURE\nMEASUREMENT\nPROCEDURE_OCCURRENCE\n\n\n\n2.3 Vocabularies and Concepts\n\nSNOMED, ICD, LOINC integration\nConcept relationships\nSource to concept mapping\nCreating custom concepts\n\n\n\n\nPractical Exercise 2\nMapping Local Data to OMOP\n\n\n\n\n\n\n\n\n\n\nLocal Field\nOMOP Table\nOMOP Field\nVocabulary\n\n\n\n\nPatient ID\nPERSON\nperson_id\n-\n\n\nBirth Date\nPERSON\nbirth_datetime\n-\n\n\nDiagnosis Code\nCONDITION_OCCURRENCE\ncondition_concept_id\nICD10CM\n\n\nLab Result\nMEASUREMENT\nvalue_as_number\nLOINC"
  },
  {
    "objectID": "workshop.html#module-3-datashield-statistical-methods",
    "href": "workshop.html#module-3-datashield-statistical-methods",
    "title": "Workshop Content",
    "section": "Module 3: DataSHIELD Statistical Methods",
    "text": "Module 3: DataSHIELD Statistical Methods\n\nLearning Objectives\n\nMaster DataSHIELD’s statistical functions\nUnderstand privacy-preserving algorithms\nPerform complex analyses\nInterpret federated results\n\n\n\nTopics Covered\n\n3.1 Descriptive Statistics\n\nSummary statistics\nContingency tables\nHistograms and density plots\nMissing data handling\n\n\n\n3.2 Statistical Modeling\n\nLinear regression\nLogistic regression\nGeneralized linear models\nMixed effects models\n\n\n\n3.3 Advanced Analytics\n\nSurvival analysis\nPrincipal component analysis\nMachine learning methods\nMeta-analysis approaches\n\n\n\n\nPractical Exercise 3\nFederated Linear Regression\n# Fit a model across multiple sites\nmodel &lt;- ds.glm(formula = \"BMI ~ AGE + GENDER + SITE\",\n                data = \"CNSIM\",\n                family = \"gaussian\",\n                datasources = connections)\n\n# View results\nprint(model)"
  },
  {
    "objectID": "workshop.html#module-4-real-world-applications",
    "href": "workshop.html#module-4-real-world-applications",
    "title": "Workshop Content",
    "section": "Module 4: Real-World Applications",
    "text": "Module 4: Real-World Applications\n\nLearning Objectives\n\nDesign multi-site studies\nHandle heterogeneous data\nAddress common challenges\nOptimize performance\n\n\n\nCase Studies\n\n4.1 Multi-Country COVID-19 Analysis\n\nHarmonizing international data\nAnalyzing outcomes across populations\nHandling missing data\nReporting standardized results\n\n\n\n4.2 Rare Disease Consortium\n\nSmall sample considerations\nPrivacy with limited data\nCross-border collaborations\nRegulatory compliance\n\n\n\n4.3 African Health Observatory\n\nInfrastructure limitations\nData quality issues\nCapacity building\nSustainable implementations\n\n\n\n\nGroup Project Options\n\nMaternal Health Outcomes: Analyze factors affecting maternal mortality across multiple African countries\nInfectious Disease Surveillance: Implement early warning system using federated data\nNCDs in Africa: Study diabetes prevalence and risk factors\nHealth Equity Analysis: Examine healthcare access disparities"
  },
  {
    "objectID": "workshop.html#module-5-advanced-topics-future-directions",
    "href": "workshop.html#module-5-advanced-topics-future-directions",
    "title": "Workshop Content",
    "section": "Module 5: Advanced Topics & Future Directions",
    "text": "Module 5: Advanced Topics & Future Directions\n\nLearning Objectives\n\nExplore cutting-edge developments\nIntegrate with other tools\nPlan for scalability\nContribute to the community\n\n\n\nTopics Covered\n\n5.1 Machine Learning in DataSHIELD\n\nPrivacy-preserving algorithms\nFederated learning approaches\nModel validation strategies\nPerformance optimization\n\n\n\n5.2 Integration & Interoperability\n\nFHIR integration\nREDCap connectivity\nWorkflow automation\nCI/CD for analyses\n\n\n\n5.3 Community & Contribution\n\nDataSHIELD packages\nContributing code\nDocumentation\nCommunity support"
  },
  {
    "objectID": "workshop.html#hands-on-labs",
    "href": "workshop.html#hands-on-labs",
    "title": "Workshop Content",
    "section": "Hands-On Labs",
    "text": "Hands-On Labs\n\nLab Environment\nEach participant will have access to: - Personal Opal server instance - Sample datasets (CNSIM, DASIM) - African health datasets (anonymized) - Jupyter notebooks with exercises - Solution keys (released daily)\n\n\nServer Connection Details\n\n\n\n\n\n\nServer Access Information\n\n\n\nConnection details will be provided at the beginning of the workshop. Each participant will receive: - Personal username and password - Server URLs for each site - VPN access if required - Support contact information\n\n\n\n\nLab Exercises by Day\nDay 1 Labs - Lab 1.1: First DataSHIELD connection - Lab 1.2: Basic data exploration - Lab 1.3: Simple statistics\nDay 2 Labs - Lab 2.1: OMOP data inspection - Lab 2.2: Vocabulary browsing - Lab 2.3: Data quality checks\nDay 3 Labs - Lab 3.1: Regression analysis - Lab 3.2: Survival analysis - Lab 3.3: Data visualization\nDay 4 Labs - Lab 4.1: Machine learning exercise - Lab 4.2: Complex queries - Lab 4.3: Performance optimization\nDay 5 Labs - Lab 5.1: Project implementation - Lab 5.2: Presentation preparation - Lab 5.3: Deployment planning"
  },
  {
    "objectID": "workshop.html#assessment-certification",
    "href": "workshop.html#assessment-certification",
    "title": "Workshop Content",
    "section": "Assessment & Certification",
    "text": "Assessment & Certification\n\nContinuous Assessment\n\nDaily lab completion (40%)\nParticipation in discussions (20%)\nGroup project (30%)\nFinal presentation (10%)\n\n\n\nCertification Requirements\n\nAttend 80% of sessions\nComplete all lab exercises\nSubmit group project\nPass final assessment"
  },
  {
    "objectID": "workshop.html#additional-resources",
    "href": "workshop.html#additional-resources",
    "title": "Workshop Content",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nReading Materials\n\nDataSHIELD User Guide\nOMOP CDM Documentation\nResearch Papers Collection\nBest Practices Guide\n\n\n\nVideo Tutorials\n\nRecorded lectures\nStep-by-step guides\nTroubleshooting tips\nExpert interviews\n\n\n\nCode Repository\nAll code examples, exercises, and solutions are available at: https://github.com/isglobal-brge/workshop_APHRC\n\n\nAccess Workshop Resources Back to Schedule"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page provides supplementary materials and resources to deepen your understanding of DataSHIELD and OMOP CDM beyond the workshop content.\n\n\n\n\n\n\n\n\nFoundational Papers: - Gaye et al. (2014) - DataSHIELD: taking the analysis to the data, not the data to the analysis - Marcon et al. (2021) - Recent advances in DataSHIELD - Wilson et al. (2017) - DataSHIELD tutorial\nTechnical Papers: - Privacy-preserving data analysis methods - Federated learning in epidemiology - Statistical disclosure control\n\n\n\n\n\n\n\n\nCore Documentation: - The Book of OHDSI - Comprehensive guide to OHDSI and OMOP - OMOP CDM v5.4 Specifications - Vocabulary Documentation\nResearch Applications: - Real-world evidence generation - Network studies methodology - Data quality frameworks\n\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD Basics - Introduction to federated analysis concepts\nOMOP CDM Deep Dive - Understanding the common data model\nR Programming for DataSHIELD - Advanced R techniques\nOHDSI Symposium Recordings - Annual conference presentations\nCommunity Webinar Series - Monthly educational sessions\n\nAccess Video Library →\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD: - dsBaseClient - dsBase - DSI\nOMOP Tools: - OHDSI/CommonDataModel - OHDSI/WhiteRabbit - OHDSI/Achilles\n\n\n\n\n\n\n\n\nAnalysis Packages:\n# DataSHIELD ecosystem\ndsBaseClient\ndsModelling\ndsSurvival\ndsMediation\n\n# OMOP utilities\nDatabaseConnector\nSqlRender\nFeatureExtraction\n\n\n\n\n\n\n\n\nExample Workflows: - Multi-site GLM analysis - Survival analysis template - Data harmonization scripts - Quality control pipelines\nDownload Templates →\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOfficial Website: datashield.org\nDiscussion Forum: Active Q&A community\nSlack Channel: Real-time support\nMailing List: datashield-user@mrc-epid.cam.ac.uk\nWorking Groups: Join specialized teams\n\nMonthly Events: - Community calls (First Tuesday) - Technical workshops - User group meetings\n\n\n\n\n\n\n\n\n\nMain Portal: ohdsi.org\nForums: forums.ohdsi.org\nMS Teams: Global collaboration\nRegional Chapters: Local networks\nAnnual Symposium: Global conference\n\nResources: - Study protocols - Phenotype library - Network studies\n\n\n\n\n\n\n\n\n\n\n\nFree Courses: - Introduction to DataSHIELD - Self-paced online course - OMOP CDM Fundamentals - OHDSI Academy - R for Epidemiologists - Coursera - Privacy-Preserving Analytics - edX\nCertification Programs: - OHDSI Certified Data Engineer - DataSHIELD Administrator Certificate - Privacy and Ethics in Health Data\nUniversity Programs: - MSc in Health Data Science - Graduate Certificate in Biomedical Informatics\n\n\n\n\n\n\n\n\n\n\n\n\nWhiteRabbit: Scan your data\nRabbit-in-a-Hat: Design ETL mappings\nUsagi: Vocabulary mapping tool\nData Quality Dashboard: DQD\nATLAS: Cohort definition tool\n\n\n\n\n\n\n\n\n\n\nSDC Tools: Statistical disclosure control\nSynthea: Synthetic patient generator\nPrivacy Audit Tools: Compliance checking\nDifferential Privacy Libraries: Google, IBM\nSecure Computing Platforms: Various options\n\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD Test Data: - CNSIM: Simulated cohort data (n=5,000) - DASIM: Disease analysis dataset - SURVIVAL: Time-to-event data\nOMOP Sample Data: - Synthea OMOP: 1K, 10K, 100K patient datasets - CMS SynPUF: Synthetic Medicare data - MIMIC-OMOP: Critical care data (requires DUA)\nAfrican Health Data: - Workshop-specific datasets - Demographic health surveys (anonymized) - Disease surveillance samples\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-site RCTs\nObservational studies\nPharmacovigilance\nComparative effectiveness\n\n\n\n\n\n\n\n\n\n\nDisease surveillance\nOutbreak investigation\nHealth disparities\nPolicy evaluation\n\n\n\n\n\n\n\n\n\n\nCost-effectiveness\nResource utilization\nHealth outcomes\nEconomic modeling\n\n\n\n\n\n\n\n\n\n\n\n\nSubscribe to stay informed about: - New DataSHIELD releases - OMOP CDM updates - Training opportunities - Community events - Research highlights\nSubscribe: workshop-updates@isglobal.org\n\n\n\n\n\n\n\n\n\n\n\n\nInternational DataSHIELD Consortium\nOHDSI Collaborative Network\nAfrican Health Data Network\nEU Health Data Space\n\nJoin existing studies or propose new collaborations!\n\n\n\n\n\n\n\n\n\nHorizon Europe calls\nNIH Big Data initiatives\nWellcome Trust programs\nAfrican research grants\nIndustry partnerships\n\nResources for proposal writing available.\n\n\n\n\n\n\n\nWhen using DataSHIELD or OMOP CDM in your research:\n\n\n\n\n\n\nHow to Cite\n\n\n\nDataSHIELD:\nGaye A, et al. DataSHIELD: taking the analysis to the data, \nnot the data to the analysis. Int J Epidemiol. 2014;43(6):1929-44.\nOMOP CDM:\nObservational Health Data Sciences and Informatics (OHDSI) Collaborative. \nThe Book of OHDSI. 2021. Available from: https://ohdsi.github.io/TheBookOfOhdsi/\n\n\n\n\n  Workshop Repository  Contact Instructors"
  },
  {
    "objectID": "resources.html#additional-learning-resources",
    "href": "resources.html#additional-learning-resources",
    "title": "Resources",
    "section": "",
    "text": "This page provides supplementary materials and resources to deepen your understanding of DataSHIELD and OMOP CDM beyond the workshop content.\n\n\n\n\n\n\n\n\nFoundational Papers: - Gaye et al. (2014) - DataSHIELD: taking the analysis to the data, not the data to the analysis - Marcon et al. (2021) - Recent advances in DataSHIELD - Wilson et al. (2017) - DataSHIELD tutorial\nTechnical Papers: - Privacy-preserving data analysis methods - Federated learning in epidemiology - Statistical disclosure control\n\n\n\n\n\n\n\n\nCore Documentation: - The Book of OHDSI - Comprehensive guide to OHDSI and OMOP - OMOP CDM v5.4 Specifications - Vocabulary Documentation\nResearch Applications: - Real-world evidence generation - Network studies methodology - Data quality frameworks\n\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD Basics - Introduction to federated analysis concepts\nOMOP CDM Deep Dive - Understanding the common data model\nR Programming for DataSHIELD - Advanced R techniques\nOHDSI Symposium Recordings - Annual conference presentations\nCommunity Webinar Series - Monthly educational sessions\n\nAccess Video Library →\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD: - dsBaseClient - dsBase - DSI\nOMOP Tools: - OHDSI/CommonDataModel - OHDSI/WhiteRabbit - OHDSI/Achilles\n\n\n\n\n\n\n\n\nAnalysis Packages:\n# DataSHIELD ecosystem\ndsBaseClient\ndsModelling\ndsSurvival\ndsMediation\n\n# OMOP utilities\nDatabaseConnector\nSqlRender\nFeatureExtraction\n\n\n\n\n\n\n\n\nExample Workflows: - Multi-site GLM analysis - Survival analysis template - Data harmonization scripts - Quality control pipelines\nDownload Templates →\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOfficial Website: datashield.org\nDiscussion Forum: Active Q&A community\nSlack Channel: Real-time support\nMailing List: datashield-user@mrc-epid.cam.ac.uk\nWorking Groups: Join specialized teams\n\nMonthly Events: - Community calls (First Tuesday) - Technical workshops - User group meetings\n\n\n\n\n\n\n\n\n\nMain Portal: ohdsi.org\nForums: forums.ohdsi.org\nMS Teams: Global collaboration\nRegional Chapters: Local networks\nAnnual Symposium: Global conference\n\nResources: - Study protocols - Phenotype library - Network studies\n\n\n\n\n\n\n\n\n\n\n\nFree Courses: - Introduction to DataSHIELD - Self-paced online course - OMOP CDM Fundamentals - OHDSI Academy - R for Epidemiologists - Coursera - Privacy-Preserving Analytics - edX\nCertification Programs: - OHDSI Certified Data Engineer - DataSHIELD Administrator Certificate - Privacy and Ethics in Health Data\nUniversity Programs: - MSc in Health Data Science - Graduate Certificate in Biomedical Informatics\n\n\n\n\n\n\n\n\n\n\n\n\nWhiteRabbit: Scan your data\nRabbit-in-a-Hat: Design ETL mappings\nUsagi: Vocabulary mapping tool\nData Quality Dashboard: DQD\nATLAS: Cohort definition tool\n\n\n\n\n\n\n\n\n\n\nSDC Tools: Statistical disclosure control\nSynthea: Synthetic patient generator\nPrivacy Audit Tools: Compliance checking\nDifferential Privacy Libraries: Google, IBM\nSecure Computing Platforms: Various options\n\n\n\n\n\n\n\n\n\n\n\n\nDataSHIELD Test Data: - CNSIM: Simulated cohort data (n=5,000) - DASIM: Disease analysis dataset - SURVIVAL: Time-to-event data\nOMOP Sample Data: - Synthea OMOP: 1K, 10K, 100K patient datasets - CMS SynPUF: Synthetic Medicare data - MIMIC-OMOP: Critical care data (requires DUA)\nAfrican Health Data: - Workshop-specific datasets - Demographic health surveys (anonymized) - Disease surveillance samples\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-site RCTs\nObservational studies\nPharmacovigilance\nComparative effectiveness\n\n\n\n\n\n\n\n\n\n\nDisease surveillance\nOutbreak investigation\nHealth disparities\nPolicy evaluation\n\n\n\n\n\n\n\n\n\n\nCost-effectiveness\nResource utilization\nHealth outcomes\nEconomic modeling\n\n\n\n\n\n\n\n\n\n\n\n\nSubscribe to stay informed about: - New DataSHIELD releases - OMOP CDM updates - Training opportunities - Community events - Research highlights\nSubscribe: workshop-updates@isglobal.org\n\n\n\n\n\n\n\n\n\n\n\n\nInternational DataSHIELD Consortium\nOHDSI Collaborative Network\nAfrican Health Data Network\nEU Health Data Space\n\nJoin existing studies or propose new collaborations!\n\n\n\n\n\n\n\n\n\nHorizon Europe calls\nNIH Big Data initiatives\nWellcome Trust programs\nAfrican research grants\nIndustry partnerships\n\nResources for proposal writing available.\n\n\n\n\n\n\n\nWhen using DataSHIELD or OMOP CDM in your research:\n\n\n\n\n\n\nHow to Cite\n\n\n\nDataSHIELD:\nGaye A, et al. DataSHIELD: taking the analysis to the data, \nnot the data to the analysis. Int J Epidemiol. 2014;43(6):1929-44.\nOMOP CDM:\nObservational Health Data Sciences and Informatics (OHDSI) Collaborative. \nThe Book of OHDSI. 2021. Available from: https://ohdsi.github.io/TheBookOfOhdsi/\n\n\n\n\n  Workshop Repository  Contact Instructors"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This page provides simple instructions to install R and all required packages for the workshop."
  },
  {
    "objectID": "getting-started.html#prerequisites-requirements",
    "href": "getting-started.html#prerequisites-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "To get the most out of this workshop, participants should have:\n\n\n\n\n\nOperating System: Windows 10+, macOS 10.14+, or Ubuntu 18.04+\nRAM: Minimum 8GB (16GB recommended)\nStorage: At least 10GB free space\nInternet: Stable broadband connection\nBrowser: Chrome, Firefox, or Safari (latest versions)\n\n\n\n\n\n\n\nBasic understanding of R programming\nFamiliarity with RStudio interface\nBasic statistical concepts\nExperience with health/clinical data (helpful but not required)\nUnderstanding of data privacy concepts"
  },
  {
    "objectID": "getting-started.html#pre-workshop-setup",
    "href": "getting-started.html#pre-workshop-setup",
    "title": "Getting Started",
    "section": "Pre-Workshop Setup",
    "text": "Pre-Workshop Setup\nComplete these steps at least 3 days before the workshop:\n\n\n\n\n\n\nEssential Setup Tasks\n\n\n\n\n1. 📊 Install R (v4.0+) and RStudio\nR Statistical Software: - Download from: CRAN - Choose the version for your operating system - Follow the installation wizard\nRStudio Desktop: - Download from: Posit - Free version is sufficient - Install after R is installed\n\n\n2. 📦 Install Required Packages\nRun this script in RStudio to install all necessary packages:\n# Install DataSHIELD packages\ninstall.packages(c(\"DSI\", \"DSOpal\", \"dsBaseClient\", \"dsHelper\"),\n                 repos = c(\"https://cran.obiba.org\", \n                          \"https://cloud.r-project.org\"))\n\n# Install additional analysis packages\ninstall.packages(c(\"tidyverse\", \"ggplot2\", \"tableone\", \n                   \"survival\", \"metafor\"))\n\n# Install OMOP CDM tools\ninstall.packages(\"DatabaseConnector\")\ninstall.packages(\"SqlRender\")\n\n# Verify installation\nlibrary(dsBaseClient)\nlibrary(DSOpal)\n\n\n3. 🔐 Test Server Access\n\nYou’ll receive credentials via email 1 week before the workshop\nTest connection using provided script\nReport any issues immediately to workshop-support@isglobal.org\n\n\n\n4. 📚 Review Pre-Reading Materials\nRequired Reading (60 minutes total): - Introduction to DataSHIELD (30 min) - OMOP CDM Overview (20 min) - Workshop Guide (10 min)\nOptional Background: - DataSHIELD: taking the analysis to the data - The Book of OHDSI"
  },
  {
    "objectID": "getting-started.html#server-connection-details",
    "href": "getting-started.html#server-connection-details",
    "title": "Getting Started",
    "section": "Server Connection Details",
    "text": "Server Connection Details\nDuring the workshop, you’ll connect to our demonstration servers:\n\n\n\n\n\n\nWorkshop Servers\n\n\n\nConnection details will be sent via email to registered participants. The workshop uses multiple servers to demonstrate federated analysis:\n\n\n\nServer\nPurpose\nURL\n\n\n\n\nDemo Server 1\nPrimary exercises\nWill be provided\n\n\nDemo Server 2\nMulti-site demonstrations\nWill be provided\n\n\nTest Server\nPractice environment\nWill be provided\n\n\nOMOP Database\nCDM exercises\nWill be provided\n\n\n\nSecurity Notice: - Never share your credentials - Use only for workshop purposes - Access will be revoked after workshop - Report any security issues immediately"
  },
  {
    "objectID": "getting-started.html#test-your-setup",
    "href": "getting-started.html#test-your-setup",
    "title": "Getting Started",
    "section": "Test Your Setup",
    "text": "Test Your Setup\nBefore the workshop, verify your installation:\n# Test script (example - servers will be activated during workshop)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\n\n# Check package versions\npackageVersion(\"dsBaseClient\")\npackageVersion(\"DSOpal\")\n\n# Test basic functionality\nds.listFunctions()\n\n\n\n\n\n\nInstallation Troubleshooting\n\n\n\nCommon Issues:\nProblem: “Package ‘xyz’ is not available”\n# Solution 1: Use alternative repository\ninstall.packages(\"xyz\", repos = \"https://cloud.r-project.org\")\n\n# Solution 2: Install from GitHub\ndevtools::install_github(\"datashield/packagename\")\nProblem: “Non-zero exit status” - Update R to latest version - Install Rtools (Windows) or Xcode (Mac) - Check internet connection\nProblem: “Cannot connect to Opal server” - Check VPN connection (if required) - Verify credentials - Test with ping command - Check firewall settings"
  },
  {
    "objectID": "getting-started.html#what-well-provide",
    "href": "getting-started.html#what-well-provide",
    "title": "Getting Started",
    "section": "What We’ll Provide",
    "text": "What We’ll Provide\nDuring the workshop, you’ll have access to:\n\nServer Access: Dedicated workshop servers with pre-loaded data\nWorkshop Materials: Slides, code, and exercises\nSample Datasets: African health data for practice\nSupport Channel: Real-time help during workshop\nCertificate: Upon successful completion"
  },
  {
    "objectID": "getting-started.html#workshop-schedule-overview",
    "href": "getting-started.html#workshop-schedule-overview",
    "title": "Getting Started",
    "section": "Workshop Schedule Overview",
    "text": "Workshop Schedule Overview\n\n\nDaily Structure\n\n09:00 - 09:30: Welcome & Coffee\n09:30 - 11:00: Morning Lecture\n11:00 - 11:15: Break\n11:15 - 12:45: Practical Session I\n12:45 - 14:00: Lunch Break\n14:00 - 15:30: Afternoon Lecture\n15:30 - 15:45: Break\n15:45 - 17:00: Practical Session II\n17:00 - 17:30: Q&A and Wrap-up\n\n\n\nWhat to Bring\n\nLaptop with required software installed\nPower adapter and chargers\nNotebook and pen\nWater bottle\nBusiness cards for networking\nQuestions about your specific use cases"
  },
  {
    "objectID": "getting-started.html#pre-workshop-orientation",
    "href": "getting-started.html#pre-workshop-orientation",
    "title": "Getting Started",
    "section": "Pre-Workshop Orientation",
    "text": "Pre-Workshop Orientation\n\n\n\n\n\n\nOptional Orientation Session\n\n\n\nJoin our pre-workshop orientation session (recommended): - Date: [To be announced] - Time: 1 hour - Format: Online via Zoom - Content: Setup verification, Q&A, meet other participants\nRegister: orientation@isglobal.org"
  },
  {
    "objectID": "getting-started.html#support-help",
    "href": "getting-started.html#support-help",
    "title": "Getting Started",
    "section": "Support & Help",
    "text": "Support & Help\n\nBefore the Workshop\n\n\nTechnical Support\n\nEmail: workshop-support@isglobal.org\nResponse Time: Within 24 hours\nAvailable: Monday-Friday, 9:00-17:00 CET\n\n\n\nGeneral Questions\n\nEmail: brge@isglobal.org\nContact: Workshop Coordinator\nPhone: +34 xxx xxx xxx\n\n\n\n\n\nDuring the Workshop\n\nReal-time Chat: Workshop Slack channel\nOffice Hours: Daily 08:00-09:00 and 17:30-18:30\nEmergency Support: Available throughout workshop hours"
  },
  {
    "objectID": "getting-started.html#final-checklist",
    "href": "getting-started.html#final-checklist",
    "title": "Getting Started",
    "section": "Final Checklist",
    "text": "Final Checklist\nBefore the workshop starts, make sure you have:\n\nR and RStudio installed and working\nAll required packages installed\nReceived and tested server credentials\nCompleted pre-reading materials\nJoined workshop communication channels\nPrepared specific questions about your use cases"
  },
  {
    "objectID": "getting-started.html#ready-to-go",
    "href": "getting-started.html#ready-to-go",
    "title": "Getting Started",
    "section": "Ready to Go!",
    "text": "Ready to Go!\nOnce you’ve completed all the setup steps, you’re ready for an amazing workshop experience. We look forward to seeing you there!"
  },
  {
    "objectID": "datashield-workshop.html#workshop-overview",
    "href": "datashield-workshop.html#workshop-overview",
    "title": "DataSHIELD Workshop",
    "section": "Workshop Overview",
    "text": "Workshop Overview\nDataSHIELD is a pioneering approach that enables researchers to analyze data across multiple sites without physically sharing individual-level data. This workshop will provide comprehensive training on implementing and using DataSHIELD for collaborative health research while maintaining the highest standards of data privacy and security.\n\nWhat You’ll Learn\n\n\n\n\n\nPrivacy-Preserving Analytics\nLearn the principles of federated analysis and how DataSHIELD enables secure computation across distributed datasets.\n\n\n\n\n\n\n\nServer Configuration\nUnderstand how to set up and configure DataSHIELD servers (Opal) and manage data access permissions.\n\n\n\n\n\n\n\n\n\nR Client Programming\nMaster the DataSHIELD R client packages and learn to write efficient federated analysis scripts.\n\n\n\n\n\n\n\nStatistical Methods\nExplore available statistical functions and learn when and how to apply them in federated settings."
  },
  {
    "objectID": "datashield-workshop.html#workshop-content",
    "href": "datashield-workshop.html#workshop-content",
    "title": "DataSHIELD Workshop",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nDay 1: Foundations\n\nIntroduction to DataSHIELD concepts\nUnderstanding the client-server architecture\nSetting up your analysis environment\nFirst federated analysis\n\n\n\nDay 2: Core Methods\n\nDescriptive statistics across sites\nData visualization in federated settings\nHandling missing data\nQuality control procedures\n\n\n\nDay 3: Advanced Analytics\n\nRegression modeling\nSurvival analysis\nMachine learning approaches\nCustom function development"
  },
  {
    "objectID": "datashield-workshop.html#prerequisites",
    "href": "datashield-workshop.html#prerequisites",
    "title": "DataSHIELD Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic R programming knowledge\nUnderstanding of statistical concepts\nFamiliarity with health data structures\nCompleted Getting Started setup"
  },
  {
    "objectID": "datashield-workshop.html#workshop-materials",
    "href": "datashield-workshop.html#workshop-materials",
    "title": "DataSHIELD Workshop",
    "section": "Workshop Materials",
    "text": "Workshop Materials\nAll materials will be provided including: - Lecture slides and notes - Hands-on exercise notebooks - Sample datasets - Code examples and solutions\n\n\nGet Started View Schedule"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Feel free to contact us with any questions, suggestions or comments!\n\n\n  \n    \n      \n      \n        \n      \n    \n    \n      Juan R González, PhD\n      DataSHIELD instructor\n\n      \n        \n        \n        \n      \n    \n  \n\n  \n    \n      \n      \n        \n      \n    \n    \n      David Sarrat González\n      OMOP CDM instructor"
  },
  {
    "objectID": "dsomop-workshop.html#workshop-overview",
    "href": "dsomop-workshop.html#workshop-overview",
    "title": "dsOMOP Workshop",
    "section": "Workshop Overview",
    "text": "Workshop Overview\nThe dsOMOP workshop focuses on the integration of DataSHIELD with the OMOP Common Data Model (CDM). This powerful combination enables federated analysis of standardized observational health data across multiple institutions while maintaining data privacy and security.\n\nWhat You’ll Learn\n\n\n\n\n\nOMOP CDM Fundamentals\nUnderstand the structure and principles of the OMOP Common Data Model and its standardized vocabularies.\n\n\n\n\n\n\n\nETL Processes\nLearn to transform local health data into OMOP CDM format using established tools and methodologies.\n\n\n\n\n\n\n\n\n\nFederated OMOP Analysis\nMaster techniques for analyzing OMOP data across multiple sites using DataSHIELD without sharing individual records.\n\n\n\n\n\n\n\nMulti-Site Studies\nDesign and implement collaborative research studies using standardized data models and federated analysis."
  },
  {
    "objectID": "dsomop-workshop.html#workshop-content",
    "href": "dsomop-workshop.html#workshop-content",
    "title": "dsOMOP Workshop",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nDay 1: OMOP CDM Introduction\n\nOMOP CDM structure and concepts\nStandardized vocabularies (SNOMED, ICD, LOINC)\nData quality assessment\nOMOP tools ecosystem\n\n\n\nDay 2: Data Transformation\n\nETL design principles\nWhiteRabbit and Rabbit-in-a-Hat\nVocabulary mapping with Usagi\nData validation procedures\n\n\n\nDay 3: Federated OMOP Analysis\n\ndsOMOP package introduction\nCohort definition across sites\nFederated descriptive analytics\nCross-site comparisons"
  },
  {
    "objectID": "dsomop-workshop.html#prerequisites",
    "href": "dsomop-workshop.html#prerequisites",
    "title": "dsOMOP Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCompletion of DataSHIELD workshop (recommended)\nUnderstanding of clinical data structures\nBasic SQL knowledge (helpful)\nCompleted Getting Started setup"
  },
  {
    "objectID": "dsomop-workshop.html#workshop-materials",
    "href": "dsomop-workshop.html#workshop-materials",
    "title": "dsOMOP Workshop",
    "section": "Workshop Materials",
    "text": "Workshop Materials\nComprehensive materials provided: - OMOP CDM documentation and guides - ETL mapping templates - dsOMOP package documentation - Real-world case studies - Practice datasets in OMOP format"
  },
  {
    "objectID": "dsomop-workshop.html#use-cases",
    "href": "dsomop-workshop.html#use-cases",
    "title": "dsOMOP Workshop",
    "section": "Use Cases",
    "text": "Use Cases\nLearn through practical examples: - Multi-site pharmacovigilance studies - Comparative effectiveness research - Disease natural history studies - Health outcomes research\n\n\nGet Started View Schedule"
  },
  {
    "objectID": "index.html#federated-standardized-analysis",
    "href": "index.html#federated-standardized-analysis",
    "title": "DataSHIELD APHRC Workshop",
    "section": "Federated standardized analysis",
    "text": "Federated standardized analysis\nThis combination of technologies enables powerful federated analysis with strong privacy guarantees:\n\nPrivacy by design: Individual-level data never leave the hosting institution.\nDisclosure control: Automated safeguards prevent re-identification via output checks.\nSecurity and governance: Role-based access, auditability, and institutional control.\nStandardization: OMOP CDM ensures consistent structure and meaning across networks.\nReproducibility: Shared code and common semantics for comparable results."
  },
  {
    "objectID": "index.html#workshop-content",
    "href": "index.html#workshop-content",
    "title": "DataSHIELD APHRC Workshop",
    "section": "Workshop content",
    "text": "Workshop content\n\n\n\nBlock 1 — Working with DataSHIELD\n\nConnect to remote nodes and manage sessions\nAssign and aggregate safely without exposing rows\nRun common statistics under disclosure thresholds\nUnderstand privacy, security, and governance in practice\n\n\n\n\nBlock 2 — OMOP CDM via dsOMOP\n\nConnect to OMOP CDM databases through DataSHIELD\nExplore standardized tables and vocabularies\nBuild cohorts and run summary queries across sites\nPerform federated, reproducible analyses on harmonized data\n\n\n\n\n\n\nGetting ready for the workshop Workshop schedule"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Workshop Schedule",
    "section": "",
    "text": "Save the Dates!\n\n\n\nWorkshop Dates: [To be announced]\nTime Zone: East Africa Time (EAT)\nFormat: Hybrid (In-person at APHRC + Virtual participation available)"
  },
  {
    "objectID": "schedule.html#workshop-dates",
    "href": "schedule.html#workshop-dates",
    "title": "Workshop Schedule",
    "section": "",
    "text": "Save the Dates!\n\n\n\nWorkshop Dates: [To be announced]\nTime Zone: East Africa Time (EAT)\nFormat: Hybrid (In-person at APHRC + Virtual participation available)"
  },
  {
    "objectID": "schedule.html#daily-schedule-overview",
    "href": "schedule.html#daily-schedule-overview",
    "title": "Workshop Schedule",
    "section": "Daily Schedule Overview",
    "text": "Daily Schedule Overview\nThe workshop follows a structured daily format designed to maximize learning while accommodating different time zones and work schedules.\n\nTypical Day Structure\n\n\n\n\nTime (EAT)\nDuration\nSession Type\n\n\n\n\n09:00 - 09:30\n30 min\nWelcome & Coffee\n\n\n09:30 - 11:00\n90 min\nMorning Lecture\n\n\n11:00 - 11:15\n15 min\nBreak\n\n\n11:15 - 12:45\n90 min\nPractical Session I\n\n\n12:45 - 14:00\n75 min\nLunch Break\n\n\n14:00 - 15:30\n90 min\nAfternoon Lecture\n\n\n15:30 - 15:45\n15 min\nBreak\n\n\n15:45 - 17:00\n75 min\nPractical Session II\n\n\n17:00 - 17:30\n30 min\nQ&A and Wrap-up"
  },
  {
    "objectID": "schedule.html#detailed-schedule",
    "href": "schedule.html#detailed-schedule",
    "title": "Workshop Schedule",
    "section": "Detailed Schedule",
    "text": "Detailed Schedule\n\nPre-Workshop Preparation\n\n\nOne Week Before Workshop\n\nInstall required software (R, RStudio, Git)\nTest connection to workshop servers\nReview pre-reading materials\nComplete pre-workshop survey\n\n\n\n\nDay 1: Introduction to Federated Analysis\n\n\n\n\n\n\n\n\n\n\nTime\nSession\nDescription\nInstructor\n\n\n\n\n09:00 - 09:30\nRegistration & Welcome\nCheck-in, networking, coffee\nAll\n\n\n09:30 - 11:00\nOpening Keynote\nThe Future of Collaborative Health Research in Africa\nTBD\n\n\n11:15 - 12:45\nIntroduction to DataSHIELD\nCore concepts, architecture, and use cases\nJuan R González\n\n\n14:00 - 15:30\nHands-on: First Steps\nConnecting to servers, basic commands\nTechnical Team\n\n\n15:45 - 17:00\nLab Session\nIndividual practice with support\nAll\n\n\n\n\n\n\nDay 2: Understanding OMOP CDM\n\n\n\n\n\n\n\n\n\n\nTime\nSession\nDescription\nInstructor\n\n\n\n\n09:30 - 11:00\nOMOP CDM Overview\nStructure, vocabularies, and concepts\nTBD\n\n\n11:15 - 12:45\nData Mapping Workshop\nETL strategies and tools\nTechnical Team\n\n\n14:00 - 15:30\nCase Study: African Health Data\nReal examples from APHRC datasets\nAPHRC Team\n\n\n15:45 - 17:00\nGroup Exercise\nMapping local data to OMOP\nAll\n\n\n\n\n\n\nDay 3: DataSHIELD Analytics\n\n\n\n\n\n\n\n\n\n\nTime\nSession\nDescription\nInstructor\n\n\n\n\n09:30 - 11:00\nStatistical Methods in DataSHIELD\nAvailable functions and limitations\nJuan R González\n\n\n11:15 - 12:45\nPractical: Descriptive Analytics\nSummary statistics, data exploration\nTechnical Team\n\n\n14:00 - 15:30\nAdvanced Analytics\nRegression, survival analysis\nTBD\n\n\n15:45 - 17:00\nLab: Your First Analysis\nComplete analysis workflow\nAll\n\n\n\n\n\n\nDay 4: Advanced Topics & Integration\n\n\n\n\n\n\n\n\n\n\nTime\nSession\nDescription\nInstructor\n\n\n\n\n09:30 - 11:00\nMachine Learning with DataSHIELD\nPrivacy-preserving ML methods\nTBD\n\n\n11:15 - 12:45\nIntegration Topics\nConnecting with other tools\nTechnical Team\n\n\n14:00 - 15:30\nProject Planning Session\nDesigning your study\nAll\n\n\n15:45 - 17:00\nGroup Project Kickoff\nForm teams, select projects\nAll\n\n\n\n\n\n\nDay 5: Projects & Future Directions\n\n\n\n\n\n\n\n\n\n\nTime\nSession\nDescription\nInstructor\n\n\n\n\n09:30 - 11:00\nProject Work Time\nTeams work on projects\nWith support\n\n\n11:15 - 12:45\nTroubleshooting Clinic\nOne-on-one help available\nTechnical Team\n\n\n14:00 - 15:30\nProject Presentations\nTeams present their work\nAll\n\n\n15:45 - 16:30\nFuture Roadmap\nWhat’s next for DataSHIELD\nJuan R González\n\n\n16:30 - 17:30\nClosing Ceremony\nCertificates, networking\nAll"
  },
  {
    "objectID": "schedule.html#special-sessions",
    "href": "schedule.html#special-sessions",
    "title": "Workshop Schedule",
    "section": "Special Sessions",
    "text": "Special Sessions\n\nLightning Talks\nThroughout the week, participants are invited to give 5-minute lightning talks about their research interests or use cases. Sign up at registration!\n\n\nOffice Hours\nDaily office hours available: - Morning: 08:00 - 09:00 (before sessions) - Evening: 17:30 - 18:30 (after sessions)\n\n\nSocial Events\n\n\n\n\n\nWelcome Reception\nDay 1, 18:00\nJoin us for an informal welcome reception to meet fellow participants and instructors.\n\n\n\n\n\n\n\nWorkshop Dinner\nDay 3, 19:00\nOptional group dinner at a local restaurant (self-pay)."
  },
  {
    "objectID": "schedule.html#virtual-participation",
    "href": "schedule.html#virtual-participation",
    "title": "Workshop Schedule",
    "section": "Virtual Participation",
    "text": "Virtual Participation\nFor those joining virtually:\n\nAll sessions will be live-streamed via Zoom\nRecorded sessions available within 24 hours\nDedicated online support channel\nVirtual networking sessions scheduled\nSame access to servers and materials\n\n\n\n\n\n\n\nTime Zone Considerations\n\n\n\n\nPrimary schedule is in East Africa Time (EAT)\nKey sessions will be recorded for asynchronous viewing\nSupport available across multiple time zones"
  },
  {
    "objectID": "schedule.html#what-to-bring",
    "href": "schedule.html#what-to-bring",
    "title": "Workshop Schedule",
    "section": "What to Bring",
    "text": "What to Bring\n\nIn-Person Participants\n\nLaptop with required software installed\nPower adapter and chargers\nNotebook and pen\nWater bottle\nBusiness cards for networking\n\n\n\nVirtual Participants\n\nStable internet connection\nHeadset with microphone\nSecond monitor recommended\nQuiet workspace"
  },
  {
    "objectID": "schedule.html#accommodation",
    "href": "schedule.html#accommodation",
    "title": "Workshop Schedule",
    "section": "Accommodation",
    "text": "Accommodation\nFor participants traveling to APHRC:\n\n\n\n\n\n\nRecommended Hotels\n\n\n\nDetails about nearby accommodation options will be provided upon registration, including: - Hotels within walking distance - Transportation options - Special workshop rates"
  },
  {
    "objectID": "schedule.html#important-reminders",
    "href": "schedule.html#important-reminders",
    "title": "Workshop Schedule",
    "section": "Important Reminders",
    "text": "Important Reminders\n\nPre-workshop Setup: Complete all setup requirements before Day 1\nPunctuality: Sessions start promptly at scheduled times\nParticipation: Active engagement expected in all sessions\nCode of Conduct: Respectful, inclusive environment for all\nHealth & Safety: Current guidelines will be followed\n\n\n\nView Workshop Content Access Resources"
  },
  {
    "objectID": "DataSHIELD_workshop_APHRC_resources.html",
    "href": "DataSHIELD_workshop_APHRC_resources.html",
    "title": "DataSHIELD Workshop: Data Science Without Borders",
    "section": "",
    "text": "Analysis using the resources\nNow, let us illustrate a similar analysis of the previous example using CNSIM datasets but having the data as a resource. Now the resources are available in a project called RSRC (see https://opal-demo.obiba.org/#/project/RSRC/resources). Now, we write all the require code in a single chunk:\n\nlibrary(DSOpal)\nlibrary(dsBaseClient)\n\n# prepare login data and resources to assign\nbuilder &lt;- DSI::newDSLoginBuilder()\nbuilder$append(server = \"study1\", url = \"https://opal-demo.obiba.org\", \n               user = \"dsuser\", password = \"P@ssw0rd\", \n               resource = \"RSRC.CNSIM1\", profile = \"default\")\n#builder$append(server = \"study2\", url = \"https://opal.isglobal.org/repo\",\n#               user = \"invited\",  password = \"12345678Aa@\",, \n#               resource = \"CNSIM.CNSIM2\", profile = \"rock-inma\")\n\nlogindata &lt;- builder$build()\n\n# login and assign resources\nconns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = \"res\")\n\n# assigned objects are of class ResourceClient (and others)\nds.class(\"res\")\n\n$study1\n[1] \"SQLResourceClient\" \"ResourceClient\"    \"R6\"               \n\n# coerce ResourceClient objects to data.frames\n# (DataSHIELD config allows as.resource.data.frame() assignment function for the purpose of the demo)\ndatashield.assign.expr(conns, symbol = \"D\", \n                       expr = quote(as.resource.data.frame(res, strict = TRUE)))\nds.class(\"D\")\n\n$study1\n[1] \"data.frame\"\n\nds.colnames(\"D\")\n\n$study1\n [1] \"id\"                 \"LAB_TSC\"            \"LAB_TRIG\"          \n [4] \"LAB_HDL\"            \"LAB_GLUC_ADJUSTED\"  \"PM_BMI_CONTINUOUS\" \n [7] \"DIS_CVA\"            \"MEDI_LPD\"           \"DIS_DIAB\"          \n[10] \"DIS_AMI\"            \"GENDER\"             \"PM_BMI_CATEGORICAL\"\n\n# do usual dsBase analysis\nds.summary('D$LAB_HDL')\n\n$study1\n$study1$class\n[1] \"numeric\"\n\n$study1$length\n[1] 2163\n\n$study1$`quantiles & mean`\n      5%      10%      25%      50%      75%      90%      95%     Mean \n0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 \n\n# vector types are not necessarily the same depending on the data reader that was used\nds.class('D$GENDER')\n\n$study1\n[1] \"integer\"\n\nds.asFactor('D$GENDER', 'GENDER')\n\n$all.unique.levels\n[1] \"0\" \"1\"\n\n$return.message\n[1] \"Data object &lt;GENDER&gt; correctly created in all specified data sources\"\n\nds.summary('GENDER')\n\n$study1\n$study1$class\n[1] \"factor\"\n\n$study1$length\n[1] 2163\n\n$study1$categories\n[1] \"0\" \"1\"\n\n$study1$`count of '0'`\n[1] 1092\n\n$study1$`count of '1'`\n[1] 1071\n\nmod &lt;- ds.glm(\"DIS_DIAB ~ LAB_TRIG + GENDER\", data = \"D\" , family=\"binomial\")\nmod$coeff\n\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -5.1696619  0.4549328 -11.363572 6.349427e-30   -6.0613138\nLAB_TRIG     0.3813891  0.1037611   3.675647 2.372471e-04    0.1780211\nGENDER      -0.2260851  0.4375864  -0.516664 6.053908e-01   -1.0837387\n            high0.95CI.LP        P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)    -4.2780099 0.005654338    0.002325913      0.01368049\nLAB_TRIG        0.5847570 1.464317247    1.194850574      1.79455494\nGENDER          0.6315685 0.797650197    0.338328242      1.88055787\n\ndatashield.logout(conns)\n\nThe Figure @ref(fig:opalOmic) describes the different types of ’omic association analyses that can be performed using DataSHIELD client functions implemented in the dsOmicsClient package. Basically, data (’omic and phenotypes/covariates) can be stored in different sites (http, ssh, AWS S3, local, …) and are managed with Opal through the resourcer package and their extensions implemented in dsOmics.\n\n\n\n\n\nNon-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how the resourcer package is used to get access to omic data through the Opal servers. Then DataSHIELD is used in the client side to perform non-disclosive data analyses.\n\n\n\n\nThe dsOmicsClient package allows different types of analyses: pooled and meta-analysis. Both methods are based on fitting different Generalized Linear Models (GLMs) for each feature when assessing association between ’omic data and the phenotype/trait/condition of interest. Of course, non-disclosive ’omic data analysis from a single study can also be performed.\nThe pooled approach (Figure @ref(fig:omicAnal1)) is recommended when the user wants to analyze ’omic data from different sources and obtain results as if the data were located in a single computer. It should be noted that this can be very time consuming when analyzing multiple features since it calls a base function in DataSHIELD (ds.glm) repeatedly. It also cannot be recommended when data are not properly harmonized (e.g. gene expression normalized using different methods, GWAS data having different platforms, …). Furthermore when it is necesary to remove unwanted variability (for transcriptomic and epigenomica analysis) or control for population stratification (for GWAS analysis), this approach cannot be used since we need to develop methods to compute surrogate variables (to remove unwanted variability) or PCAs (to to address population stratification) in a non-disclosive way.\nThe meta-analysis approach Figure @ref(fig:omicAnal2) overcomes the limitations raised when performing pooled analyses. First, the computation issue is addressed by using scalable and fast methods to perform data analysis at whole-genome level at each location The transcriptomic and epigenomic data analyses make use of the widely used limma package that uses ExpressionSet or RangedSummarizedExperiment Bioc infrastructures to deal with ’omic and phenotypic (e.g covariates). The genomic data are analyzed using GWASTools and GENESIS that are designed to perform quality control (QC) and GWAS using GDS infrastructure.\nNext, we describe how both approaches are implemented:\n\nPooled approach: Figure @ref(fig:omicAnal1) illustrate how this analysis is performed. This corresponds to generalized linear models (glm) on data from single or multiple sources. It makes use of ds.glm() function which is a DataSHIELD function that uses an approach that is mathematically equivalent to placing all individual-level data froma all sources in one central warehouse and analysing those data using the conventional glm() function in R. The user can select one (or multiple) features (i.e., genes, transcripts, CpGs, SNPs, …)\n\n\n\n\n\n\nNon-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform single pooled omic data analysis. The analyses are performed by using a generalized linear model (glm) on data from one or multiple sources. It makes use of ds.glm(), a DataSHIELD function, that uses an approach that is mathematically equivalent to placing all individual-level data from all sources in one central warehouse and analysing those data using the conventional glm() function in R.\n\n\n\n\n\nMeta-analysis: Figure @ref(fig:omicAnal2) illustrate how this analysis is performed. This corresponds to performing a genome-wide analysis at each location using functions that are specifically design for that purpose and that are scalable. Then the results from each location can be meta-analyzed using methods that meta-analyze either effect sizes or p-values.\n\n\n\n\n\n\nNon-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform anlyses at genome-wide level from one or multiple sources. It runs standard Bioconductor functions at each server independently to speed up the analyses and in the case of having multiple sources, results can be meta-analyzed uning standar R functions.\n\n\n\n\n\n\nDifferential gene expression (DGE) analysis\nDon’t forget to log out! Using:\n\ndatashield.logout(conns)\n\n\n\nExercise\nTo be supplied"
  },
  {
    "objectID": "getting-started.html#setup-instructions",
    "href": "getting-started.html#setup-instructions",
    "title": "Getting Started",
    "section": "",
    "text": "This page provides simple instructions to install R and all required packages for the workshop."
  },
  {
    "objectID": "getting-started.html#step-1-install-r-and-rstudio",
    "href": "getting-started.html#step-1-install-r-and-rstudio",
    "title": "Getting Started",
    "section": "Step 1: Install R and RStudio",
    "text": "Step 1: Install R and RStudio\n\nInstall R\n\nGo to CRAN\nDownload R for your operating system (Windows, Mac, or Linux)\nRun the installer and follow the instructions\n\n\n\nInstall RStudio\n\nGo to Posit RStudio\nDownload the free RStudio Desktop\nInstall RStudio after R is installed"
  },
  {
    "objectID": "getting-started.html#step-2-install-required-r-packages",
    "href": "getting-started.html#step-2-install-required-r-packages",
    "title": "Getting Started",
    "section": "Step 2: Install Required R Packages",
    "text": "Step 2: Install Required R Packages\nOpen RStudio and run the following commands to install all necessary packages:\n# Core DataSHIELD packages\ninstall.packages(\"DSI\", repos = c(\"https://cran.obiba.org\", \"https://cloud.r-project.org\"))\ninstall.packages(\"DSOpal\", repos = c(\"https://cran.obiba.org\", \"https://cloud.r-project.org\"))\ninstall.packages(\"dsBaseClient\", repos = c(\"https://cran.obiba.org\", \"https://cloud.r-project.org\"))\n\n# Install devtools for GitHub installations\ninstall.packages(\"devtools\")\n\n# DataSHIELD helper packages\ndevtools::install_github(\"timcadman/ds-helper\")\n\n# dsOMOP packages\ndevtools::install_github('isglobal-brge/dsOMOPClient')\ndevtools::install_github('isglobal-brge/dsOMOPHelper')\n\n# Statistical analysis packages\ninstall.packages(\"metafor\")\n\n# Basic data manipulation (if not already installed)\ninstall.packages(\"readr\")\ninstall.packages(\"knitr\")\n\n# Additional packages for advanced workshops (optional)\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"dsOmicsClient\", ask = FALSE)"
  },
  {
    "objectID": "getting-started.html#step-3-verify-installation",
    "href": "getting-started.html#step-3-verify-installation",
    "title": "Getting Started",
    "section": "Step 3: Verify Installation",
    "text": "Step 3: Verify Installation\nRun this code to check that packages are installed correctly:\n# Load main packages\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\nlibrary(dsHelper)\nlibrary(metafor)\n\n# Check DataSHIELD functions are available\nprint(\"DataSHIELD packages loaded successfully!\")\n\n# Check package versions\ncat(\"Package versions:\\n\")\ncat(\"DSI:\", as.character(packageVersion(\"DSI\")), \"\\n\")\ncat(\"DSOpal:\", as.character(packageVersion(\"DSOpal\")), \"\\n\")\ncat(\"dsBaseClient:\", as.character(packageVersion(\"dsBaseClient\")), \"\\n\")"
  },
  {
    "objectID": "getting-started.html#troubleshooting",
    "href": "getting-started.html#troubleshooting",
    "title": "Getting Started",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Installation Issues\nIf a package fails to install:\n# Try installing from CRAN only\ninstall.packages(\"package_name\", repos = \"https://cloud.r-project.org\")\n\n# Or try updating R if packages require newer version\n# Then reinstall the package\nFor GitHub packages that fail:\n# Make sure devtools is installed and loaded\ninstall.packages(\"devtools\")\nlibrary(devtools)\n\n# If still failing, try:\ninstall.packages(\"remotes\")\nremotes::install_github(\"username/repository\")\nOn Windows: You may need to install Rtools from CRAN Rtools\nOn Mac: You may need Xcode Command Line Tools. Install by running in Terminal:\nxcode-select --install"
  },
  {
    "objectID": "getting-started.html#workshop-information",
    "href": "getting-started.html#workshop-information",
    "title": "Getting Started",
    "section": "Workshop Information",
    "text": "Workshop Information\n\nServer Access\nConnection details and credentials will be provided at the beginning of the workshop.\n\n\nSupport\n\nEmail: workshop-support@isglobal.org\nGeneral questions: brge@isglobal.org\n\n\n\nView Workshop Schedule Contact Instructors"
  }
]